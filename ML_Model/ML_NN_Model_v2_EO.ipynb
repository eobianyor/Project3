{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DEPENDENCIES\n",
    "# FOR DATA\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "# import requests\n",
    "# import datefinder\n",
    "\n",
    "# # FOR SQL LITE\n",
    "# from sqlalchemy import create_engine\n",
    "# from datetime import date\n",
    "\n",
    "# # FOR PLOTTING\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"fivethirtyeight\")\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# FOR MODELING\n",
    "from scipy.optimize import curve_fit\n",
    "# from splinter import Browser\n",
    "# from bs4 import BeautifulSoup as BS\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the CSV Files\n",
    "idahoFireWeatherDrought = os.path.join(\"..\", \"Data\", \"fires_Idaho_2000_2015_drought_weather.csv\")\n",
    "\n",
    "# Open the CSV Files, Convert to a Dataframe, and Save as a Variable\n",
    "idaho_Fire_Weather_Drought_df = pd.read_csv(idahoFireWeatherDrought)\n",
    "# fires_Idaho_df = pd.read_csv(idahoFires, dtype={\"LOCAL_INCIDENT_ID\": 'string', \"FIRE_NAME\": 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime format\n",
    "idaho_Fire_Weather_Drought_df['DISCOVERY_DATE_CONVERTED'] = pd.to_datetime(idaho_Fire_Weather_Drought_df['DISCOVERY_DATE_CONVERTED'])\n",
    "idaho_Fire_Weather_Drought_df['CONT_DATE_CONVERTED'] = pd.to_datetime(idaho_Fire_Weather_Drought_df['CONT_DATE_CONVERTED'])\n",
    "\n",
    "idaho_Fire_Weather_Drought_df['DISCOVERY_DATE_CONVERTED']=idaho_Fire_Weather_Drought_df['DISCOVERY_DATE_CONVERTED'].apply(lambda x: int(x.strftime('%m')))\n",
    "# View the Data in the Dataframe\n",
    "# print(idaho_Fire_Weather_df.keys())\n",
    "idaho_Fire_Weather_Drought_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLEAN UP AND REMOVE UNWANTED COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get dummy variables for nominal property column\n",
    "# # idaho_Fire_Weather_df = pd.get_dummies(idaho_Fire_Weather_df, columns=[\"FIRE_SIZE_CLASS\"])\n",
    "# idaho_Fire_Weather_Drought_df = pd.get_dummies(idaho_Fire_Weather_Drought_df, columns=[\"CITY\"])\n",
    "# idaho_Fire_Weather_Drought_df = pd.get_dummies(idaho_Fire_Weather_Drought_df, columns=[\"NAME\"])\n",
    "\n",
    "# # FIRE_SIZE_CLASS_NOS  = {'A' : 1, 'B' : 2, 'C' : 3, 'D' : 4, 'E' : 5, 'F' : 6, 'G' : 7}\n",
    "\n",
    "# # # replace values in each column according to the dictionaries above\n",
    "# # clean_fires_Idaho_2000_2015_df.replace({'FIRE_SIZE_CLASS': FIRE_SIZE_CLASS_NOS}, inplace=True) \n",
    "                    \n",
    "# idaho_Fire_Weather_Drought_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummy variables for nominal property column\n",
    "# idaho_Fire_Weather_Drought_df = pd.get_dummies(idaho_Fire_Weather_Drought_df, columns=[\"STAT_CAUSE_DESCR\"])\n",
    "# idaho_Fire_Weather_Drought_df = pd.get_dummies(idaho_Fire_Weather_Drought_df, columns=[\"NAME\"])\n",
    "\n",
    "# FIRE_SIZE_CLASS_NOS  = {'A' : 1, 'B' : 2, 'C' : 3, 'D' : 4, 'E' : 5, 'F' : 6, 'G' : 7}\n",
    "\n",
    "# # replace values in each column according to the dictionaries above\n",
    "# idaho_Fire_Weather_Drought_df.replace({'FIRE_SIZE_CLASS': FIRE_SIZE_CLASS_NOS}, inplace=True) \n",
    "                    \n",
    "# idaho_Fire_Weather_Drought_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and associate cities using the lat lng coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idaho_Fire_Weather_Drought_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Avg for values\n",
    "Day1_prcp = idaho_Fire_Weather_Drought_df['DAY_PRCP_1'].mean()\n",
    "Day2_prcp = idaho_Fire_Weather_Drought_df['DAY_PRCP_2'].mean()\n",
    "Day3_prcp = idaho_Fire_Weather_Drought_df['DAY_PRCP_3'].mean()\n",
    "Day4_prcp = idaho_Fire_Weather_Drought_df['DAY_PRCP_4'].mean()\n",
    "Day1_temp = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_1'].mean()\n",
    "Day2_temp = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_2'].mean()\n",
    "Day3_temp = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_3'].mean()\n",
    "Day4_temp = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_4'].mean()\n",
    "\n",
    "# Use Avg values to fill any null values\n",
    "idaho_Fire_Weather_Drought_df['DAY_PRCP_1'] = idaho_Fire_Weather_Drought_df['DAY_PRCP_1'].fillna(Day1_prcp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_PRCP_2'] = idaho_Fire_Weather_Drought_df['DAY_PRCP_2'].fillna(Day2_prcp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_PRCP_3'] = idaho_Fire_Weather_Drought_df['DAY_PRCP_3'].fillna(Day3_prcp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_PRCP_4'] = idaho_Fire_Weather_Drought_df['DAY_PRCP_4'].fillna(Day4_prcp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_1'] = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_1'].fillna(Day1_temp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_2'] = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_2'].fillna(Day2_temp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_3'] = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_3'].fillna(Day3_temp)\n",
    "idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_4'] = idaho_Fire_Weather_Drought_df['DAY_AVG_TEMP_4'].fillna(Day4_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corr_df = idaho_Fire_Weather_Drought_df[['FIRE_SIZE_CLASS', 'DISCOVERY_DATE_CONVERTED', 'FIRE_YEAR', 'STAT_CAUSE_DESCR', 'AVE_SIZE12', 'CROP_ACR12', 'NAME', 'None', 'D0','D1', 'D2', 'D3', 'D4', \n",
    "                                   'DAY_PRCP_1', 'DAY_PRCP_2', 'DAY_PRCP_3', 'DAY_PRCP_4', 'DAY_AVG_TEMP_1', 'DAY_AVG_TEMP_2', 'DAY_AVG_TEMP_3', 'DAY_AVG_TEMP_4']]\n",
    "\n",
    "# Corr_df = idaho_Fire_Weather_Drought_df[['NAME', 'None', 'D0','D1', 'D2', 'D3', 'D4', \n",
    "#                                    'DAY_PRCP_1', 'DAY_PRCP_2', 'DAY_PRCP_3', 'DAY_PRCP_4', 'DAY_AVG_TEMP_1', 'DAY_AVG_TEMP_2', 'DAY_AVG_TEMP_3', 'DAY_AVG_TEMP_4']]\n",
    "    \n",
    "Corr_df['FIRE_SIZE_CLASS']= Corr_df['FIRE_SIZE_CLASS'].astype('category').cat.codes\n",
    "\n",
    "Corr_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A NEURAL NETWORK MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVESTIGATING INPUTS\n",
    "# Possible X columns\n",
    "# [['DISCOVERY_DATE_CONVERTED', 'FIRE_SIZE_CLASS', 'AVE_FAM_SZ', 'NO_FARMS12', 'AVE_SIZE12', 'CROP_ACR12', 'NAME', 'None', 'D0','D1', 'D2', 'D3', 'D4', 'DAY_PRCP_1', 'DAY_PRCP_2', 'DAY_PRCP_3', \n",
    "#   'DAY_PRCP_4', 'DAY_AVG_TEMP_1', 'DAY_AVG_TEMP_2', 'DAY_AVG_TEMP_3', 'DAY_AVG_TEMP_4']]\n",
    "\n",
    "New_df = idaho_Fire_Weather_Drought_df[['FIRE_SIZE_CLASS', 'STAT_CAUSE_DESCR', 'DISCOVERY_DATE_CONVERTED', 'NAME', 'None', 'D0','D1', 'D2', 'D3', 'D4', \n",
    "                                   'DAY_PRCP_1', 'DAY_PRCP_2', 'DAY_PRCP_3', 'DAY_PRCP_4', 'DAY_AVG_TEMP_1', 'DAY_AVG_TEMP_2', 'DAY_AVG_TEMP_3', 'DAY_AVG_TEMP_4']]\n",
    "New_df['FIRE_SIZE_CLASS']= New_df['FIRE_SIZE_CLASS'].astype('category').cat.codes\n",
    "# New_df['AVE_FAM_SZ']= New_df['AVE_FAM_SZ'].apply(lambda x: x//1)\n",
    "\n",
    "# Drop Y column\n",
    "New_df = New_df.drop(['FIRE_SIZE_CLASS'], axis=1)\n",
    "New_df = New_df.drop(['STAT_CAUSE_DESCR'], axis=1)\n",
    "\n",
    "# Run PCA \n",
    "from sklearn.decomposition import PCA\n",
    "n_components=40\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Create multiple columns for County \"NAME\"\n",
    "New_df = pd.get_dummies(New_df, columns=['NAME'])\n",
    "# New_df = pd.get_dummies(New_df, columns=['STAT_CAUSE_DESCR'])\n",
    "\n",
    "NoOfCols = n_components\n",
    "\n",
    "X_Array = New_df.to_numpy()\n",
    "pca.fit(X_Array)\n",
    "# print(pca.singular_values_)\n",
    "x = pca.transform(X_Array)\n",
    "x\n",
    "# print(x.shape)\n",
    "# type(x)\n",
    "# x\n",
    "\n",
    "NoOfCols = n_components\n",
    "NoOfRuns = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CREATE X VALUES\n",
    "# # X by Keep\n",
    "# X = idaho_Fire_Weather_Drought_df[['STAT_CAUSE_DESCR', 'DISCOVERY_DATE_CONVERTED', 'NAME', 'None', 'D0','D1', 'D2', 'D3', 'D4', 'DAY_PRCP_1', 'DAY_PRCP_2', 'DAY_PRCP_3', 'DAY_PRCP_4', 'DAY_AVG_TEMP_1', 'DAY_AVG_TEMP_2', 'DAY_AVG_TEMP_3', 'DAY_AVG_TEMP_4']]\n",
    "\n",
    "# Either Or\n",
    "# X = X.drop(['STAT_CAUSE_DESCR'], axis=1)\n",
    "# X = pd.get_dummies(X, columns=['STAT_CAUSE_DESCR'])\n",
    "# X = pd.get_dummies(X, columns=['NAME'])\n",
    "\n",
    "# NoOfCols = 82\n",
    "# NoOfRuns = 100\n",
    "# X = X.values.reshape(-1, NoOfCols)\n",
    "\n",
    "# # X = X.to_numpy()\n",
    "\n",
    "# print(X.shape)\n",
    "# type(X)\n",
    "# # X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE y VALUES\n",
    "y = idaho_Fire_Weather_Drought_df[['STAT_CAUSE_DESCR']]\n",
    "\n",
    "# y = idaho_Fire_Weather_Drought_df[['STAT_CAUSE_DESCR', 'FIRE_SIZE_CLASS']]\n",
    "# y = pd.get_dummies(y, columns=[\"STAT_CAUSE_DESCR\"])\n",
    "\n",
    "# y = y.values.reshape(-1, 2)\n",
    "\n",
    "# print(y.shape)\n",
    "# # type(y)\n",
    "# y\n",
    "\n",
    "# LABEL ENCODE Y\n",
    "# Import required module\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Create an object of the label encoder class\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "# apply \"le.fit_transform\"\n",
    "old_y = y.apply(le.fit_transform)\n",
    "y = old_y\n",
    "\n",
    "# # Change the shape of y v1\n",
    "new_y = np.array(old_y)\n",
    "y = new_y.reshape(-1, 1) \n",
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST AND TRIAN SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale your data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "# # y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "# # Create variables to hold the scaled train & test data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "# # y_train_scaled = y_scaler.transform(y_train)\n",
    "# # y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode the categorical target variable to the necessary format for the model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 2 neural network with 1 hidden layer and 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Inputs\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X Inputs\n",
    "print(X_train.shape)\n",
    "print(X_train_scaled.shape)\n",
    "\n",
    "# Y Inputs\n",
    "print(y_train_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal neural network with X inputs, 1 hidden layer, 10 nodes in hidden layer, and 7 outputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "fire_cause_model_v1 = Sequential()\n",
    "fire_cause_model_v1.add(Dense(units=20, activation='sigmoid', input_dim=NoOfCols))\n",
    "fire_cause_model_v1.add(Dense(128, activation='relu'))\n",
    "fire_cause_model_v1.add(Dense(128, activation='relu'))\n",
    "fire_cause_model_v1.add(Dropout(.1))\n",
    "fire_cause_model_v1.add(Dense(units=13, activation='softmax'))\n",
    "\n",
    "# view the model's architecture\n",
    "fire_cause_model_v1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile and train the deep learning model\n",
    "fire_cause_model_v1.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "modelHistory_v1 = fire_cause_model_v1.fit(\n",
    "    X_train,\n",
    "    y_train_categorical,\n",
    "    epochs=NoOfRuns,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MODEL ACCURACY\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,4))\n",
    "\n",
    "ax[0].plot(modelHistory_v1.history['loss'])\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_title(\"Loss\")\n",
    "\n",
    "ax[1].plot(modelHistory_v1.history['accuracy'])\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = fire_cause_model_v1.evaluate(X_test, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making Predictions with new data\n",
    "# new_data = X_scaler.transform(np.array([[-1.2, 0.3, 0.4]]))\n",
    "new_data = X_test\n",
    "\n",
    "print(f\"Model output: {fire_cause_model_v1.predict(new_data)}\")\n",
    "print(f\"Predicted class: {np.argmax(fire_cause_model_v1.predict(new_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep neural network with X inputs, 2 hidden layers, 10 nodes in each hidden layer, and 7 outputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "fire_cause_model_v2 = Sequential()\n",
    "fire_cause_model_v2.add(Dense(units=20, activation='sigmoid', input_dim=NoOfCols))\n",
    "fire_cause_model_v2.add(Dense(units=128, activation='sigmoid'))\n",
    "fire_cause_model_v2.add(Dense(units=128, activation='sigmoid'))\n",
    "fire_cause_model_v2.add(Dense(units=128, activation='sigmoid'))\n",
    "fire_cause_model_v2.add(Dense(units=13, activation='softmax'))\n",
    "\n",
    "# view the model's architecture\n",
    "fire_cause_model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile and train the deep learning model\n",
    "fire_cause_model_v2.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "modelHistory_v2 = fire_cause_model_v2.fit(\n",
    "    X_train,\n",
    "    y_train_categorical,\n",
    "    epochs=NoOfRuns,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MODEL ACCURACY\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,4))\n",
    "\n",
    "ax[0].plot(modelHistory_v2.history['loss'])\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_title(\"Loss\")\n",
    "\n",
    "ax[1].plot(modelHistory_v2.history['accuracy'])\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_title(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Model v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = fire_cause_model_v2.evaluate(X_test, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Making Predictions with new data\n",
    "# new_data = X_scaler.transform(np.array([[-1.2, 0.3, 0.4]]))\n",
    "new_data = X_test\n",
    "\n",
    "print(f\"Model output: {fire_cause_model_v2.predict(new_data)}\")\n",
    "print(f\"Predicted class: {np.argmax(fire_cause_model_v2.predict(new_data))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### View prediction probabilities\n",
    "predictions_v1 = np.argmax(fire_cause_model_v1.predict(X_test), axis=1)\n",
    "probs_v1 = fire_cause_model_v1.predict(X_test)\n",
    "\n",
    "predictions_v2 = np.argmax(fire_cause_model_v2.predict(X_test), axis=1)\n",
    "probs_v2 = fire_cause_model_v2.predict(X_test)\n",
    "\n",
    "# Change the shape of y\n",
    "old_y_test = y_test\n",
    "new_y_test = np.array(old_y_test)\n",
    "y_test = new_y_test.reshape(-1, 1) \n",
    "y_test\n",
    "y_test_df = y_test.ravel()\n",
    "y_test_df\n",
    "\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"Predictions_v1\": predictions_v1,\n",
    "    \"Predictions_v2\": predictions_v2,\n",
    "    \"Actual\": y_test_df, \n",
    "    \"P(0) model1\": np.round(probs_v1[:, 0], 5),\n",
    "    \"P(100) model1\": np.round(probs_v1[:, 1], 5),\n",
    "    \"P(0) model2\": np.round(probs_v2[:, 0], 5),\n",
    "    \"P(100) model2\": np.round(probs_v2[:, 1], 5)\n",
    "    })\n",
    "\n",
    "pred_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = fire_cause_model_v1.evaluate(X_test, y_test_categorical, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "model_loss, model_accuracy = fire_cause_model_v2.evaluate(X_test, y_test_categorical, verbose=2)\n",
    "print(f\"Deep Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model by updating \"your_name\" with your name\n",
    "# and \"your_model\" with your model variable\n",
    "# be sure to turn this in to BCS\n",
    "# if joblib fails to import, try running the command to install in terminal/git-bash\n",
    "import joblib\n",
    "filename1 = 'NN_Model_v1.sav'\n",
    "joblib.dump(fire_cause_model_v1, filename1)\n",
    "\n",
    "filename2 = 'NN_Model_v2.sav'\n",
    "joblib.dump(fire_cause_model_v2, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
