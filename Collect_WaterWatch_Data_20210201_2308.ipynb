{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# ---------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# ---------------------------------------------------------\n",
    "from splinter import Browser\n",
    "# ---------------------------------------------------------\n",
    "from bs4 import BeautifulSoup as BS\n",
    "# ---------------------------------------------------------\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "# ---------------------------------------------------------\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "# ---------------------------------------------------------\n",
    "from itertools import dropwhile\n",
    "# ---------------------------------------------------------\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "\n",
    "\n",
    "\n",
    "# import warnings\n",
    "# import pymongo\n",
    "# import requests\n",
    "# import datefinder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs for the Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USGS_WaterWatch_url = \"https://waterwatch.usgs.gov/index.php?id=wwdrought\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 00 - Scrape the Tables from USGS' WaterWatch Retrieval Summary of 7-day Flow Conditions Using Pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This did not work b/c the table is created with JavaScript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing the USGS' WaterWatch Retrieval Summary of 7-day Flow Conditions Websites\n",
    "# River_Stream_7Day_Flow_Conditions_Tables = pd.read_html(\"https://waterwatch.usgs.gov/index.php?id=wwdrought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(River_Stream_7Day_Flow_Conditions_Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# River_Stream_7Day_Flow_Conditions_Tables[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Scraping the Website with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a webpage and create a BeautifulSoup object from the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 USGS' WaterWatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Retrieve the data/information on USGS' WaterWatch website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # identify location of chromedriver and store it as a variable\n",
    "# chromedriver = !which chromedriver\n",
    "# print(type(chromedriver))\n",
    "# chromedriver[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve page with the requests module\n",
    "# executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "# # OR\n",
    "# # executable_path = {\"executable_path\": chromedriver[0]}\n",
    "# # I am not sure why the above works and the below statement will not. I think it's b/c chromebriver is a class 'IPython.utils.text.SList'?\n",
    "# # executable_path = {\"executable_path\": chromedriver}\n",
    "\n",
    "# browser = Browser('chrome', **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of page to be scraped\n",
    "# url = \"https://waterwatch.usgs.gov/index.php?id=wwdrought\"\n",
    "# browser.visit(url)\n",
    "# # window = browser.windows.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = browser.html\n",
    "# soup = BS(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the html code of the NASA's Mars website\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Click FULL IMAGE to see a large thumbnail of the featured image \n",
    "# browser.click_link_by_id('st')\n",
    "# # browser.fill('st', \"Idaho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/19392466/python-beautifulsoup-get-select-value-not-text\n",
    "\n",
    "# for option in soup.find_all('option'):\n",
    "#     print(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from selenium import webdriver\n",
    "# # from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.get(USGS_WaterWatch_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "\n",
    "# select = Select(driver.find_element_by_id('st'))\n",
    "\n",
    "# # Select by visible text\n",
    "# select.select_by_visible_text('Idaho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill in Input Fills\n",
    "# #     - https://stackoverflow.com/questions/25537567/how-to-open-website-and-fill-in-input-using-selenium-webdriver\n",
    "# # Clear the Input Field\n",
    "# #     - http://10minbasics.com/clear-fill-input-field-with-selenium/\n",
    "\n",
    "# element = driver.find_element_by_name(\"bdt\")\n",
    "# element.clear()\n",
    "# element.send_keys(\"1990-01-01\")\n",
    "\n",
    "# element = driver.find_element_by_name(\"edt\")\n",
    "# element.clear()\n",
    "# element.send_keys(\"1990-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Press/Click a Button Without an ID\n",
    "# #     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "\n",
    "# NEXT_BUTTON_XPATH = '//input[@type=\"submit\" and @value=\"GO\"]'\n",
    "\n",
    "# button = driver.find_element_by_xpath(NEXT_BUTTON_XPATH)\n",
    "# button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class\n",
    "\n",
    "# # test = soup.find_all(\"div\", class_= \"ztable\")\n",
    "# # test\n",
    "\n",
    "# # https://stackoverflow.com/questions/20522820/how-to-get-tbody-from-table-from-python-beautiful-soup\n",
    "# soup.findAll('table')[0].findAll('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Grab All Page Source on the Page\n",
    "# soup_lxml = BS(driver.page_source, \"lxml\")\n",
    "\n",
    "# # Find All the Tables on the Page\n",
    "# tables = soup_lxml.find_all(\"table\")\n",
    "# tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the Tables with Pandas\n",
    "# dfs = pd.read_html(str(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Access the Table\n",
    "# print(f\"Number of Tables on the page: {len(dfs)}\")\n",
    "# print(\"*********************************************************************************************************\")\n",
    "# print(f\"Data Types for the Table: {dfs[11].dtypes}\")\n",
    "# print(\"*********************************************************************************************************\")\n",
    "# print(f\"Number of Rows in the dataframe: {len(dfs[11])}\")\n",
    "# dfs[11].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfs[11][\"USGSstationnumber\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests, json\n",
    "# text = requests.get(\"https://waterwatch.usgs.gov/index.php?id=wwdrought\").text\n",
    "# data = json.loads(text)\n",
    "# print(data['Scty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tr in soup.find_all('tr')[2:]:\n",
    "#     tds = tr.find_all('td')\n",
    "#     print (tds)#\"Nome: %s, Cognome: %s, Email: %s\" % \\\n",
    "# #           (tds[0].text, tds[1].text, tds[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scrape a Table of All the Streamflow Stations In Idaho by County, Save a CSV File with Daily Streamflow Data for Each Station, and Scrape a Table with Extended Streamflow Statistics for Each Streamflow Station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a webpage and create a BeautifulSoup object from the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create the Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "\n",
    "url = \"https://waterwatch.usgs.gov/index.php?id=wwdrought\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/12323403/how-do-i-find-an-element-that-contains-specific-text-in-selenium-webdriver-pyth\n",
    "# https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[contains(text(), 'Current Streamflow')]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52873433/python-selenium-clicking-based-on-alt-attribute\n",
    "\n",
    "driver.find_element_by_css_selector('[alt=\"id\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press/Click a Button Without an ID\n",
    "#     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "\n",
    "lst_all_statns = '//input[@type=\"radio\" and @value=\"statelist\"]'\n",
    "\n",
    "button = driver.find_element_by_xpath(lst_all_statns)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "\n",
    "select = Select(driver.find_element_by_id('select_display'))\n",
    "\n",
    "# Select by visible text\n",
    "# select.select_by_visible_text('Daily Stage and Streamflow')\n",
    "\n",
    "# Select by value text\n",
    "select.select_by_value('dailystagedischarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "\n",
    "select = Select(driver.find_element_by_id('group_table_by'))\n",
    "\n",
    "# Select by visible text\n",
    "# select.select_by_visible_text('Daily Stage and Streamflow')\n",
    "\n",
    "# Select by value text\n",
    "select.select_by_value('county_cd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press/Click a Button Without an ID\n",
    "#     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "\n",
    "sbmt_bttn = '//input[@type=\"submit\" and @value=\"go\"]'\n",
    "\n",
    "button = driver.find_element_by_xpath(sbmt_bttn)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find All the Station Numbers Scrap the table with all the stations and use that table to loop \n",
    "# through and click each station's link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "crrnt_url = driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tables on the current page: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationNumber</th>\n",
       "      <th>Station name</th>\n",
       "      <th>Dailymeangage height(ft)3/5</th>\n",
       "      <th>Dailymeanstream- flow (ft3/s)3/5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada County</td>\n",
       "      <td>Ada County</td>\n",
       "      <td>Ada County</td>\n",
       "      <td>Ada County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13206000</td>\n",
       "      <td>BOISE RIVER AT GLENWOOD BRIDGE NR BOISE ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13206305</td>\n",
       "      <td>BOISE RIVER SOUTH CHANNEL AT EAGLE ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13206400</td>\n",
       "      <td>EAGLE DRAIN AT EAGLE, ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bannock County</td>\n",
       "      <td>Bannock County</td>\n",
       "      <td>Bannock County</td>\n",
       "      <td>Bannock County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>13011500</td>\n",
       "      <td>PACIFIC CREEK AT MORAN, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>13011900</td>\n",
       "      <td>BUFFALO FORK AB LAVA CREEK NR MORAN WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>13013650</td>\n",
       "      <td>SNAKE RIVER AT MOOSE, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>13015000</td>\n",
       "      <td>GROS VENTRE RIVER AT ZENITH, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>13018750</td>\n",
       "      <td>SNAKE RIVER BELOW FLAT CREEK, NEAR JACKSON, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      StationNumber                                    Station name  \\\n",
       "0        Ada County                                      Ada County   \n",
       "1          13206000      BOISE RIVER AT GLENWOOD BRIDGE NR BOISE ID   \n",
       "2          13206305           BOISE RIVER SOUTH CHANNEL AT EAGLE ID   \n",
       "3          13206400                        EAGLE DRAIN AT EAGLE, ID   \n",
       "4    Bannock County                                  Bannock County   \n",
       "..              ...                                             ...   \n",
       "271        13011500                      PACIFIC CREEK AT MORAN, WY   \n",
       "272        13011900          BUFFALO FORK AB LAVA CREEK NR MORAN WY   \n",
       "273        13013650                        SNAKE RIVER AT MOOSE, WY   \n",
       "274        13015000                 GROS VENTRE RIVER AT ZENITH, WY   \n",
       "275        13018750  SNAKE RIVER BELOW FLAT CREEK, NEAR JACKSON, WY   \n",
       "\n",
       "    Dailymeangage height(ft)3/5 Dailymeanstream- flow (ft3/s)3/5  \n",
       "0                    Ada County                       Ada County  \n",
       "1                           NaN                              281  \n",
       "2                           NaN                              247  \n",
       "3                           NaN                             6.60  \n",
       "4                Bannock County                   Bannock County  \n",
       "..                          ...                              ...  \n",
       "271                         NaN                               --  \n",
       "272                         NaN                               --  \n",
       "273                         NaN                              998  \n",
       "274                         NaN                               --  \n",
       "275                         NaN                             1380  \n",
       "\n",
       "[276 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class\n",
    "\n",
    "statn_table = pd.read_html(crrnt_url)\n",
    "print(f\"Number of Tables on the current page: {len(statn_table)}\")\n",
    "statn_table[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(statn_table[1].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df = statn_table[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station name</th>\n",
       "      <th>Dailymeangage height(ft)3/5</th>\n",
       "      <th>Dailymeanstream- flow (ft3/s)3/5</th>\n",
       "      <th>numbers</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada County</td>\n",
       "      <td>Ada County</td>\n",
       "      <td>Ada County</td>\n",
       "      <td></td>\n",
       "      <td>Ada County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOISE RIVER AT GLENWOOD BRIDGE NR BOISE ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>281</td>\n",
       "      <td>13206000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOISE RIVER SOUTH CHANNEL AT EAGLE ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247</td>\n",
       "      <td>13206305</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EAGLE DRAIN AT EAGLE, ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.60</td>\n",
       "      <td>13206400</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bannock County</td>\n",
       "      <td>Bannock County</td>\n",
       "      <td>Bannock County</td>\n",
       "      <td></td>\n",
       "      <td>Bannock County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>PACIFIC CREEK AT MORAN, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>13011500</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>BUFFALO FORK AB LAVA CREEK NR MORAN WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>13011900</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>SNAKE RIVER AT MOOSE, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998</td>\n",
       "      <td>13013650</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>GROS VENTRE RIVER AT ZENITH, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>--</td>\n",
       "      <td>13015000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>SNAKE RIVER BELOW FLAT CREEK, NEAR JACKSON, WY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1380</td>\n",
       "      <td>13018750</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Station name  \\\n",
       "0                                        Ada County   \n",
       "1        BOISE RIVER AT GLENWOOD BRIDGE NR BOISE ID   \n",
       "2             BOISE RIVER SOUTH CHANNEL AT EAGLE ID   \n",
       "3                          EAGLE DRAIN AT EAGLE, ID   \n",
       "4                                    Bannock County   \n",
       "..                                              ...   \n",
       "271                      PACIFIC CREEK AT MORAN, WY   \n",
       "272          BUFFALO FORK AB LAVA CREEK NR MORAN WY   \n",
       "273                        SNAKE RIVER AT MOOSE, WY   \n",
       "274                 GROS VENTRE RIVER AT ZENITH, WY   \n",
       "275  SNAKE RIVER BELOW FLAT CREEK, NEAR JACKSON, WY   \n",
       "\n",
       "    Dailymeangage height(ft)3/5 Dailymeanstream- flow (ft3/s)3/5   numbers  \\\n",
       "0                    Ada County                       Ada County             \n",
       "1                           NaN                              281  13206000   \n",
       "2                           NaN                              247  13206305   \n",
       "3                           NaN                             6.60  13206400   \n",
       "4                Bannock County                   Bannock County             \n",
       "..                          ...                              ...       ...   \n",
       "271                         NaN                               --  13011500   \n",
       "272                         NaN                               --  13011900   \n",
       "273                         NaN                              998  13013650   \n",
       "274                         NaN                               --  13015000   \n",
       "275                         NaN                             1380  13018750   \n",
       "\n",
       "               text  \n",
       "0        Ada County  \n",
       "1                    \n",
       "2                    \n",
       "3                    \n",
       "4    Bannock County  \n",
       "..              ...  \n",
       "271                  \n",
       "272                  \n",
       "273                  \n",
       "274                  \n",
       "275                  \n",
       "\n",
       "[276 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate the text from the digits in the \"StationNumber\" column.\n",
    "# https://stackoverflow.com/questions/56851679/how-to-separate-pandas-column-that-contains-values-stored-as-text-and-numbers-in\n",
    "\n",
    "statn_table_df_splt_StatnNmbr = statn_table_df.join(statn_table_df.pop('StationNumber').str.extract('(?P<numbers>\\d+)?(?P<text>\\D+)?').fillna(''))\n",
    "statn_table_df_splt_StatnNmbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NEW CELL\n",
    "\n",
    "# if statn_table_df_splt_StatnNmbr[\"text\"][1] == \"\":\n",
    "#     print(\"No County\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with only the \"numbers\" column from the \"statn_table_df_splt\" Dataframe\n",
    "statn_table_df_splt_StatnNmbr_nmbrs_clmn = pd.DataFrame(statn_table_df_splt_StatnNmbr[\"numbers\"])\n",
    "\n",
    "# Replace the Empty Rows with \"NaN\"\n",
    "# https://www.kite.com/python/answers/how-to-drop-empty-rows-from-a-pandas-dataframe-in-python\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "statn_table_df_splt_StatnNmbr_nmbrs_clmn.replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the Number of Null Values in the Dataframe\n",
    "# # https://stackoverflow.com/questions/26266362/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe\n",
    "\n",
    "# statn_table_df_splt_StatnNmbr_nmbrs_clmn.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Null Values: numbers    0\n",
      "dtype: int64\n",
      "*********************************************************************************************************\n",
      "numbers    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove the \"NaN\" Null Values\n",
    "statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls = statn_table_df_splt_StatnNmbr_nmbrs_clmn.dropna()\n",
    "print(f\"Number of Null Values: {statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls.isna().sum()}\")\n",
    "print(\"*********************************************************************************************************\")\n",
    "print(statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Convert the \"numbers\" column to an Interger Data Type\n",
    "\n",
    "# statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls[\"numbers\"] = statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls[\"numbers\"].astype(int)\n",
    "\n",
    "# print(statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgn_date = \"1990-01-01\"\n",
    "end_date = \"1990-01-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr\n",
    "\n",
    "for row in statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls.index:\n",
    "    \n",
    "    # Find the Hyper Link for One Station\n",
    "    statn_nmbr = statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls[\"numbers\"][row]\n",
    "\n",
    "\n",
    "    # https://stackoverflow.com/questions/32874539/using-a-variable-in-xpath-in-python-selenium\n",
    "    # driver.find_element_by_xpath(\"//*[contains(text(), '13206000')]\").click()\n",
    "    driver.find_element_by_xpath(\"//*[contains(text(),'\" +statn_nmbr+\"')]\").click()\n",
    "\n",
    "\n",
    "    # Select the Tab-separated Output format\n",
    "    lst_all_statns = '//input[@type=\"radio\" and @value=\"rdb\"]'\n",
    "\n",
    "    lst_all_statns_button = driver.find_element_by_xpath(lst_all_statns)\n",
    "    lst_all_statns_button.click()\n",
    "\n",
    "\n",
    "    # Enter Values for the Begin Date and End Date\n",
    "    # Fill in Input Fills\n",
    "    #     - https://stackoverflow.com/questions/25537567/how-to-open-website-and-fill-in-input-using-selenium-webdriver\n",
    "    # Clear the Input Field\n",
    "    #     - http://10minbasics.com/clear-fill-input-field-with-selenium/\n",
    "\n",
    "    element = driver.find_element_by_name(\"begin_date\")\n",
    "    element.clear()\n",
    "    element.send_keys(bgn_date)\n",
    "\n",
    "    element = driver.find_element_by_name(\"end_date\")\n",
    "    element.clear()\n",
    "    element.send_keys(end_date)\n",
    "\n",
    "    # Press/Click a Button Without an ID\n",
    "    #     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "    #     - https://stackoverflow.com/questions/21322116/using-selenium-in-python-to-click-select-a-radio-button/21322160\n",
    "\n",
    "    sbmt_bttn = '//input[@id=\"go_available_button\"]'\n",
    "\n",
    "    sbmt_bttn_button = driver.find_element_by_xpath(sbmt_bttn)\n",
    "    sbmt_bttn_button.click()\n",
    "\n",
    "\n",
    "#     from selenium.webdriver import ActionChains\n",
    "\n",
    "    actionChains = ActionChains(driver)\n",
    "\n",
    "\n",
    "    # Save the data file to This Computer\n",
    "\n",
    "    # How to Open and Write to a File on This Computer\n",
    "    #     - https://programminghistorian.org/en/lessons/working-with-web-pages\n",
    "    # How to Change the Location of the File\n",
    "    #     - https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python_FileText.html\n",
    "\n",
    "#     import urllib.request, urllib.error, urllib.parse\n",
    "#     import os\n",
    "\n",
    "\n",
    "    response = urllib.request.urlopen(driver.current_url)\n",
    "    webContent = response.read()\n",
    "\n",
    "    fle_nm = \"Data/Idaho_Streamflow_Data/\" + statn_nmbr + \".txt\"\n",
    "\n",
    "    f = open(fle_nm, 'wb')\n",
    "    f.write(webContent)\n",
    "    f.close\n",
    "\n",
    "# Go back to the original URL for the station\n",
    "    driver.back()\n",
    "    \n",
    "# Select the \"Time-series: Current/Historical Observations\" from the dropdown list, this will \n",
    "# create page which includes a table with extended streamflow statistics.\n",
    "\n",
    "    crrnt_hstrcl_obsrvtns = '//input[@value=\"uv\"]'\n",
    "    \n",
    "    select = Select(driver.find_element_by_id(\"select_data_1\"))\n",
    "    select.select_by_visible_text(\"Time-series:   Current/Historical Observations\")\n",
    "    \n",
    "# Get the extended year streamflow min, max, median, mean, 25th percentile, and 75th percentile\n",
    "    # https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class\n",
    "    \n",
    "    crrnt_hstrcl_obsrvtns_url = driver.current_url\n",
    "    crrnt_hstrcl_obsrvtns_html = pd.read_html(crrnt_hstrcl_obsrvtns_url)\n",
    "    print(f'Station Number: {statn_nmbr}')\n",
    "    print(f\"Number of Tables on the current page: {len(crrnt_hstrcl_obsrvtns_html)}\")\n",
    "#     print(type(crrnt_hstrcl_obsrvtns_html[1]))\n",
    "    \n",
    "    if len(crrnt_hstrcl_obsrvtns_html) == 2:\n",
    "        \n",
    "#         print(crrnt_hstrcl_obsrvtns_html[1])\n",
    "\n",
    "        extndd_yrs_sttstcs = crrnt_hstrcl_obsrvtns_html[1]\n",
    "\n",
    "\n",
    "        # Reference: \n",
    "    #     - Find column whose name contains a specific string:\n",
    "    #         - https://stackoverflow.com/questions/21285380/find-column-whose-name-contains-a-specific-string\n",
    "\n",
    "    # print(extndd_yrs_sttstcs.columns)\n",
    "        extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if 'Min' in col_nm]\n",
    "        min_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "#         print(f'Min Flow: {min_strmflw}')\n",
    "\n",
    "        extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if 'Max' in col_nm]\n",
    "        max_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "#         print(f'Max Flow: {max_strmflw}')\n",
    "\n",
    "        extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if '25th' in col_nm]\n",
    "        _25th_prcntle_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "#         print(f'25th_prcntle: {_25th_prcntle_strmflw}')\n",
    "\n",
    "        extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if '75th' in col_nm]\n",
    "        _75th_prcntle_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "#         print(f'75th_prcntle: {_75th_prcntle_strmflw}')\n",
    "\n",
    "\n",
    "\n",
    "            # References:\n",
    "        #     - Return the Index label if some condition is satisfied over a column in Pandas Dataframe:\n",
    "        #         - geeksforgeeks.org/return-the-index-label-if-some-condition-is-satisfied-over-a-column-in-pandas-dataframe/\n",
    "        #     - Pandas update a cell:\n",
    "        #         - https://kanoki.org/2019/04/12/pandas-how-to-get-a-cell-value-and-update-it/\n",
    "\n",
    "\n",
    "        # Find the Index for the Station  \n",
    "        indx_lbl = statn_table_df_splt_StatnNmbr[statn_table_df_splt_StatnNmbr[\"numbers\"] == statn_nmbr].index.tolist()\n",
    "        print(f'Index No: {indx_lbl[0]}')\n",
    "\n",
    "        # Append the Extened Water Years Average to the \"statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls\"\n",
    "        # Dataframe\n",
    "    # #     statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_min\": indx_lbl}, ignore_index=True)\n",
    "        statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_min\"] = extndd_yrs_sttstcs[min_strmflw][0]\n",
    "\n",
    "    #     statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_max\": indx_lbl}, ignore_index=True)\n",
    "        statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_max\"] = extndd_yrs_sttstcs[max_strmflw][0]\n",
    "\n",
    "    #     statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_median\": indx_lbl}, ignore_index=True)\n",
    "        statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_median\"] = extndd_yrs_sttstcs[\"Median\"][0]\n",
    "\n",
    "    #     statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_mean\": indx_lbl}, ignore_index=True)\n",
    "        statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_mean\"] = extndd_yrs_sttstcs[\"Mean\"][0]\n",
    "\n",
    "    #     statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"25th_prcntle\": indx_lbl}, ignore_index=True)\n",
    "        statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"25th_prcntle\"] = extndd_yrs_sttstcs[_25th_prcntle_strmflw][0]\n",
    "\n",
    "    #     statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"75th_prcntle\": indx_lbl}, ignore_index=True)\n",
    "        statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"75th_prcntle\"] = extndd_yrs_sttstcs[_75th_prcntle_strmflw][0]\n",
    "    \n",
    "    elif len(crrnt_hstrcl_obsrvtns_html) < 2:\n",
    "        print(\"No Extened Water Statistics\")\n",
    "    \n",
    "    print(\"********************************************************************************\")\n",
    "    \n",
    "# Go back to the URL with the list of Stations and Counties\n",
    "    driver.back()\n",
    "    driver.back()\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_splt_StatnNmbr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium.webdriver import ActionChains\n",
    "\n",
    "# actionChains = ActionChains(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the data file to This Computer\n",
    "\n",
    "# # How to Open and Write to a File on This Computer\n",
    "# #     - https://programminghistorian.org/en/lessons/working-with-web-pages\n",
    "# # How to Change the Location of the File\n",
    "# #     - https://www3.ntu.edu.sg/home/ehchua/programming/webprogramming/Python_FileText.html\n",
    "\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# import os\n",
    "\n",
    "# # url='https://waterdata.usgs.gov/id/nwis/dv?cb_00060=on&format=rdb&site_no=13206000&referred_module=sw&period=&begin_date=1990-01-01&end_date=1990-12-31'\n",
    "\n",
    "# response = urllib.request.urlopen(driver.current_url)\n",
    "# webContent = response.read()\n",
    "\n",
    "# output_fle_nm = \"Data/Idaho_Streamflow_Data/\" + statn_nmbr + \".txt\"\n",
    "\n",
    "# f = open(output_fle_nm, 'wb')\n",
    "# f.write(webContent)\n",
    "# f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.back()\n",
    "# driver.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crrnt_hstrcl_obsrvtns_url = driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class\n",
    "\n",
    "# crrnt_hstrcl_obsrvtns_html = pd.read_html(crrnt_hstrcl_obsrvtns_url)\n",
    "# print(f\"Number of Tables on the current page: {len(crrnt_hstrcl_obsrvtns_html)}\")\n",
    "# print(type(crrnt_hstrcl_obsrvtns_html[1]))\n",
    "\n",
    "# extndd_yrs_sttstcs = crrnt_hstrcl_obsrvtns_html[1]\n",
    "# extndd_yrs_sttstcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reference: \n",
    "# #     - Find column whose name contains a specific string:\n",
    "# #         - https://stackoverflow.com/questions/21285380/find-column-whose-name-contains-a-specific-string\n",
    "\n",
    "# print(extndd_yrs_sttstcs.columns)\n",
    "# extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if 'Min' in col_nm]\n",
    "# min_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "\n",
    "# extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if 'Max' in col_nm]\n",
    "# max_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "\n",
    "# extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if '25th' in col_nm]\n",
    "# _25th_prcntle_strmflw = extndd_yrs_sttstcs_cols[0]\n",
    "\n",
    "# extndd_yrs_sttstcs_cols = [col_nm for col_nm in extndd_yrs_sttstcs.columns if '75th' in col_nm]\n",
    "# _75th_prcntle_strmflw = extndd_yrs_sttstcs_cols[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using the Median because I was the most drastic/losest streamflow\n",
    "# extndd_yrs_sttstcs[\"Median\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # References:\n",
    "# #     - Return the Index label if some condition is satisfied over a column in Pandas Dataframe:\n",
    "# #         - geeksforgeeks.org/return-the-index-label-if-some-condition-is-satisfied-over-a-column-in-pandas-dataframe/\n",
    "# #     - Pandas update a cell:\n",
    "# #         - https://kanoki.org/2019/04/12/pandas-how-to-get-a-cell-value-and-update-it/\n",
    "\n",
    "# statn_nmbr = \"13206305\"\n",
    "\n",
    "# # Find the Index for the Station  \n",
    "# indx_lbl = statn_table_df_splt_StatnNmbr[statn_table_df_splt_StatnNmbr[\"numbers\"] == statn_nmbr].index.tolist()\n",
    "# print(indx_lbl[0])\n",
    "\n",
    "# # Append the Extened Water Years Average to the \"statn_table_df_splt_StatnNmbr_nmbrs_clmn_no_nulls\"\n",
    "# # Dataframe\n",
    "# # statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_min\": indx_lbl}, ignore_index=True)\n",
    "# statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_min\"] = extndd_yrs_sttstcs[min_strmflw][0]\n",
    "\n",
    "# # statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_max\": indx_lbl}, ignore_index=True)\n",
    "# statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_max\"] = extndd_yrs_sttstcs[max_strmflw][0]\n",
    "\n",
    "# # statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_median\": indx_lbl}, ignore_index=True)\n",
    "# statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_median\"] = extndd_yrs_sttstcs[\"Median\"][0]\n",
    "\n",
    "# # statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"extndd_yrs_mean\": indx_lbl}, ignore_index=True)\n",
    "# statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"extndd_yrs_mean\"] = extndd_yrs_sttstcs[\"Mean\"][0]\n",
    "\n",
    "# # statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"25th_prcntle\": indx_lbl}, ignore_index=True)\n",
    "# statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"25th_prcntle\"] = extndd_yrs_sttstcs[_25th_prcntle_strmflw][0]\n",
    "\n",
    "# # statn_table_df_splt_StatnNmbr_test = statn_table_df_splt_StatnNmbr.append({\"75th_prcntle\": indx_lbl}, ignore_index=True)\n",
    "# statn_table_df_splt_StatnNmbr_test.at[indx_lbl[0], \"75th_prcntle\"] = extndd_yrs_sttstcs[_75th_prcntle_strmflw][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_splt_StatnNmbr_test.head(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Create a Dataframe for Each Station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a webpage and create a BeautifulSoup object from the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 USGS' Science for a Changing World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++\n",
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to Skip All of the Commented Lines in the File\n",
    "#     https://cmdlinetips.com/2018/01/3-ways-to-read-a-file-and-skip-initial-comments-in-python/\n",
    "\n",
    "def is_comment(s):\n",
    "    \"\"\" function to check if a line\n",
    "         starts with some character.\n",
    "         Here # for comment\n",
    "    \"\"\"\n",
    "    # return true if a line starts with #\n",
    "    return s.startswith('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Names for the Dataframe\n",
    "clmn_nms = [\"agency\", \n",
    "            \"site_nmbr\", \n",
    "            \"date\", \n",
    "            \"streamflow_rate\", \n",
    "            \"approved/pending\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Path for the Streamflow Data\n",
    "input_fle_path = os.path.join(\"Data\", \"Idaho_Streamflow_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of Data Types to Change in the Dataframe\n",
    "convert_dict = {\n",
    "                \"streamflow_rate\": float\n",
    "               } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of Files in a Directory\n",
    "#     - https://careerkarma.com/blog/python-list-files-in-directory/\n",
    "\n",
    "input_fle_lst = os.listdir(input_fle_path)\n",
    "print(input_fle_lst)\n",
    "# input_fle_nm = input_fle_lst[0]\n",
    "# print(input_fle_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How to Ignore Hidden Files\n",
    "#     - https://stackoverflow.com/questions/15235823/how-to-ignore-hidden-files-in-python-functions\n",
    "\n",
    "# Create a Dictionary of List from the Files in the Directory\n",
    "input_fle_dict = {\"Station_Nmbr\":[], \"File_Name\": [], \"df_Name\": []}\n",
    "\n",
    "for input_fle_nm in input_fle_lst:\n",
    "#     Skipping the Files that Start With \".\"\n",
    "    if not input_fle_nm.startswith('.') and os.path.isfile(os.path.join(input_fle_path, input_fle_nm)):\n",
    "\n",
    "# Append to the Station Numbers and File Names and create and append the Dataframe Name to the \n",
    "# Directory\n",
    "        input_fle_dict[\"Station_Nmbr\"].append(input_fle_nm[:-4])\n",
    "        input_fle_dict[\"File_Name\"].append(input_fle_nm)\n",
    "        input_fle_dict[\"df_Name\"].append(\"_\" + input_fle_nm[:-4] + \"_df\")\n",
    "#         print(input_fle_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_splt_StatnNmbr.head()\n",
    "\n",
    "# The Last Index Value in the \"text\" Column\n",
    "lst_row_text_clmn = statn_table_df_splt_StatnNmbr[\"text\"].last_valid_index()\n",
    "lst_row_text_clmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count_1 = 0\n",
    "\n",
    "# for row in statn_table_df_splt_StatnNmbr[\"text\"]:\n",
    "# #     print(row)\n",
    "    \n",
    "#     if row != \"\":\n",
    "# #         print(row)\n",
    "#         count_1 = count_1 + 1\n",
    "# #         cnty_lst.append(row)\n",
    "        \n",
    "# print(count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnty_lst = [{}for x in range(51)]\n",
    "# # cnty_lst[2].append(2050)\n",
    "# # cnty_lst[1]\n",
    "# cnty_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a list of list\n",
    "# #     - https://stackoverflow.com/questions/8713620/appending-items-to-a-list-of-lists-in-python\n",
    "\n",
    "\n",
    "# cnty_lst = [[]for x in range(51)]\n",
    "\n",
    "# count_1 = 0\n",
    "# count_2 = 0\n",
    "# count_3 = 0\n",
    "\n",
    "# statn_lst = []\n",
    "# # cnty_lst = [[] * 154]\n",
    "# new_dict = {}\n",
    "\n",
    "\n",
    "# for row in statn_table_df_splt_StatnNmbr[\"text\"]:\n",
    "# #     print(row)\n",
    " \n",
    "# # *********************************************************************************************\n",
    "# #                               Step 1: If the row is Not Empty\n",
    "# # *********************************************************************************************\n",
    "#     if row != \"\":\n",
    "#         print(row)\n",
    "# #         count_1 = count_1\n",
    "#         cnty_lst[count_2].append(row)\n",
    "#         cnty_lst[count_2].append([])\n",
    "# # *********************************************************************************************\n",
    "\n",
    "\n",
    "# # *********************************************************************************************\n",
    "# #                   Step 2: If the row is Empty and the Next Row is Empty\n",
    "# # *********************************************************************************************\n",
    "\n",
    "#     elif row == \"\" and statn_table_df_splt_StatnNmbr[\"text\"][count_1 + 1] == \"\":\n",
    "#         print(statn_table_df_splt_StatnNmbr[\"numbers\"][count_1]) \n",
    "# #         cnty_lst[count_2][1].append(statn_table_df_splt_StatnNmbr[\"numbers\"][count_1])\n",
    "        \n",
    "        \n",
    "# # # #         print(statn_table_df_splt_StatnNmbr[\"numbers\"][count])\n",
    "# # # #         count_3 = count_1\n",
    "# # # #         print (count_3)\n",
    "# # #         count_1 = 0\n",
    "\n",
    "\n",
    "#         new_dict = {\"Station_Nmbr\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1], \n",
    "#                                   \"File_Name\":statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \".txt\", #input_fle_nm, \n",
    "#                                   \"df_Name\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \"_df\",#\"_\" + input_fle_nm[:-4] + \"_df\", \n",
    "#                                   \"Data\": \"\", \n",
    "#                                   \"Avg_Streamflow\": \"\",\n",
    "#                                   \"Prcnt_Below_Avg\": \"\"}\n",
    "    \n",
    "#         cnty_lst[count_2][1].append(dict(new_dict))\n",
    "# # *********************************************************************************************\n",
    "\n",
    "\n",
    "# # *********************************************************************************************\n",
    "# #                   Step 3: If the row is Empty and the Next Row is Not Empty\n",
    "# # *********************************************************************************************\n",
    "#     elif row == \"\" and statn_table_df_splt_StatnNmbr[\"text\"][count_1 + 1] != \"\":\n",
    "# #         cnty_lst[count_2][1].append(statn_table_df_splt_StatnNmbr[\"numbers\"][count_1])\n",
    "        \n",
    "#         new_dict = {\"Station_Nmbr\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1], \n",
    "#                                   \"File_Name\":statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \".txt\", \n",
    "#                                   \"df_Name\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \"_df\",\n",
    "#                                   \"Data\": \"\", \n",
    "#                                   \"Avg_Streamflow\": \"\",\n",
    "#                                   \"Prcnt_Below_Avg\": \"\"}\n",
    "    \n",
    "#         cnty_lst[count_2][1].append(dict(new_dict))\n",
    "        \n",
    "#         count_2 = count_2 + 1\n",
    "# # *********************************************************************************************\n",
    "    \n",
    "    \n",
    "# # *********************************************************************************************\n",
    "# #                               Step 4: Add 1 to the Count\n",
    "# # *********************************************************************************************    \n",
    "#     count_1 = count_1 + 1\n",
    "# # *********************************************************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # statn_lst\n",
    "\n",
    "# # cnty_lst[0][0].append(input_fle_dict)\n",
    "# # cnty_lst[0][1].append(\"test\")\n",
    "# # print(cnty_lst)\n",
    "# print(cnty_lst[0])\n",
    "# print(\"********************************************************************\")\n",
    "# print(cnty_lst[0][1])\n",
    "# print(\"********************************************************************\")\n",
    "# print(cnty_lst[0][1][0])\n",
    "# print(\"********************************************************************\")\n",
    "# print(cnty_lst[0][1][0][\"Station_Nmbr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_splt_StatnNmbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "# Create a List of the Counties\n",
    "cnty_lst = []\n",
    "\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "\n",
    "# Create a List of the Stations\n",
    "# statn_lst = []\n",
    "# cnty_lst = [[] * 154]\n",
    "new_dict = {}\n",
    "\n",
    "# The \"text\" Column is the County Name\n",
    "for row in statn_table_df_splt_StatnNmbr[\"text\"]:\n",
    "#     print(row)\n",
    " \n",
    "#     If this count is <= the Last Row in the \"text\" Column\n",
    "    if count_1 + 1 < 276:\n",
    "# *********************************************************************************************\n",
    "#                               Step 1: If the row is Not Empty\n",
    "# *********************************************************************************************\n",
    "        if row != \"\":\n",
    "            # Create a List of the Stations\n",
    "            statn_lst = []  # This will be a list of dictionaries\n",
    "            cnty_nm = row\n",
    "#         print(row)\n",
    "# #         count_1 = count_1\n",
    "#         cnty_lst[count_2].append(row)\n",
    "#         cnty_lst[count_2].append([])\n",
    "# *********************************************************************************************\n",
    "\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                   Step 2: If the row is Empty and the Next Row is Empty\n",
    "# *********************************************************************************************\n",
    "\n",
    "        elif row == \"\" and statn_table_df_splt_StatnNmbr[\"text\"][count_1 + 1] == \"\":\n",
    "#         print(statn_table_df_splt_StatnNmbr[\"numbers\"][count_1]) \n",
    "#         cnty_lst[count_2][1].append(statn_table_df_splt_StatnNmbr[\"numbers\"][count_1])\n",
    "        \n",
    "        \n",
    "# # #         print(statn_table_df_splt_StatnNmbr[\"numbers\"][count])\n",
    "# # #         count_3 = count_1\n",
    "# # #         print (count_3)\n",
    "# #         count_1 = 0\n",
    "\n",
    "\n",
    "            new_dict = {\"Station_Nmbr\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1], \n",
    "                        \"File_Name\":statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \".txt\", #input_fle_nm, \n",
    "                        \"df_Name\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \"_df\",#\"_\" + input_fle_nm[:-4] + \"_df\", \n",
    "                        \"Data\": \"\", \n",
    "                        \"Avg_Streamflow\": \"\",\n",
    "                        \"Prcnt_Below_Avg\": \"\"}\n",
    "    \n",
    "            statn_lst.append(dict(new_dict))\n",
    "# *********************************************************************************************\n",
    "\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                   Step 3: If the row is Empty and the Next Row is Not Empty\n",
    "# *********************************************************************************************\n",
    "        elif row == \"\": #and statn_table_df_splt_StatnNmbr[\"text\"][count_1 + 1] != \"\":\n",
    "#         cnty_lst[count_2][1].append(statn_table_df_splt_StatnNmbr[\"numbers\"][count_1])\n",
    "        \n",
    "            new_dict = {\"Station_Nmbr\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1], \n",
    "                        \"File_Name\":statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \".txt\", \n",
    "                        \"df_Name\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \"_df\",\n",
    "                        \"Data\": \"\", \n",
    "                        \"Avg_Streamflow\": \"\",\n",
    "                        \"Prcnt_Below_Avg\": \"\"}\n",
    "    \n",
    "            statn_lst.append(dict(new_dict))\n",
    "        \n",
    "#         count_2 = count_2 + 1\n",
    "        \n",
    "            cnty_lst.append(dict({cnty_nm: statn_lst}))\n",
    "        \n",
    "# *********************************************************************************************\n",
    "\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                   Step 3: If the row is Empty and the Next Row is Not Empty\n",
    "# *********************************************************************************************  \n",
    "    if count_1 + 1 > 275 and row == \"\":\n",
    "        print (count_1)\n",
    "\n",
    "\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                   Step 3: If the row is Empty and the Next Row is Not Empty\n",
    "# *********************************************************************************************\n",
    "        \n",
    "        new_dict = {\"Station_Nmbr\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1], \n",
    "                    \"File_Name\":statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \".txt\", \n",
    "                    \"df_Name\": statn_table_df_splt_StatnNmbr[\"numbers\"][count_1] + \"_df\",\n",
    "                    \"Data\": \"\", \n",
    "                    \"Avg_Streamflow\": \"\",\n",
    "                    \"Prcnt_Below_Avg\": \"\"}\n",
    "\n",
    "        statn_lst.append(dict(new_dict))\n",
    "\n",
    "        cnty_lst.append(dict({cnty_nm: statn_lst}))\n",
    "# *********************************************************************************************\n",
    "    \n",
    "    \n",
    "# *********************************************************************************************\n",
    "#                               Step 4: Add 1 to the Count\n",
    "# *********************************************************************************************    \n",
    "    count_1 = count_1 + 1\n",
    "# *********************************************************************************************\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(cnty_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# statn_lst\n",
    "# cnty_lst = {cnty_nm: statn_lst}\n",
    "# cnty_lst[0][0].append(input_fle_dict)\n",
    "# cnty_lst[0][1].append(\"test\")\n",
    "# pprint.pprint(cnty_lst)\n",
    "# print(\"********************************************************************\")\n",
    "# print(new_dict)\n",
    "# print(statn_lst)\n",
    "pprint.pprint(cnty_lst[0])\n",
    "# pprint.pprint(cnty_lst[50])\n",
    "print(\"********************************************************************\")\n",
    "pprint.pprint(cnty_lst[0][\"Ada County\"])\n",
    "# pprint.pprint(cnty_lst[50][\"Teton County, Wyoming\"])\n",
    "print(\"********************************************************************\")\n",
    "pprint.pprint(cnty_lst[0][\"Ada County\"][0])\n",
    "# pprint.pprint(cnty_lst[50][\"Teton County, Wyoming\"][0])\n",
    "print(\"********************************************************************\")\n",
    "pprint.pprint(cnty_lst[0][\"Ada County\"][0][\"Station_Nmbr\"])\n",
    "# pprint.pprint(cnty_lst[50][\"Teton County, Wyoming\"][0][\"Station_Nmbr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python-get-dictionary-keys-as-a-list/\n",
    "\n",
    "print(cnty_lst[0].keys())\n",
    "print(\"********************************************************************\")\n",
    "print(cnty_lst[0][\"Ada County\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_fle_path_fr_lp = input_fle_path + \"/\" + cnty_lst[0][key_list[0]][0][\"File_Name\"]\n",
    "# # input_fle_path_fr_lp = input_fle_path + \"/\" + statn_data_lst_of_dicts[i]['File_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I DON'T THINK I NEED THIS!!!\n",
    "\n",
    "# key_list = list(cnty_lst[0].keys())\n",
    "# val_list = list(cnty_lst[0].values())\n",
    "# key_list[0]\n",
    "# print(cnty_lst[0][key_list[0]][0][\"File_Name\"])\n",
    "\n",
    "# input_fle_path + \"/\" + cnty_lst[0][key_list[0]][0][\"File_Name\"]\n",
    "# # val_list\n",
    "\n",
    "# # position = val_list.index(1)\n",
    "\n",
    "\n",
    "# # *********************************************************************************************\n",
    "# # *********************************************************************************************\n",
    "# #     Create the Dataframe to Store the Streamflow Data from the .txt File\n",
    "# df = pd.DataFrame(columns = clmn_nms)\n",
    "# #     df_nm = \"_\" + statn_nm + \"_df\"\n",
    "# # *********************************************************************************************\n",
    "\n",
    "# # *********************************************************************************************\n",
    "# #                           Step 2 Create a Dataframe for the Streamflow\n",
    "# # *********************************************************************************************\n",
    "# # Loop Through the .txt File and Store the Data into a Dataframe\n",
    "# with open(input_fle_path_fr_lp,'r') as fh:\n",
    "#     for curline in dropwhile(is_comment, fh):\n",
    "#     #         print(f\"Index Number: {count} {curline}\")\n",
    "#     #         count = count + 1\n",
    "\n",
    "\n",
    "\n",
    "#     # Split a String\n",
    "#     #     - https://www.geeksforgeeks.org/python-string-split/\n",
    "#     # Pandas Series\n",
    "#     #     - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "#         to_append = curline[:-1].split(\"\\t\")\n",
    "#         a_series = pd.Series(to_append, index = clmn_nms)\n",
    "\n",
    "#     #             Dataframe\n",
    "#     #             Dataframe Name\n",
    "#         statn_nm = input_fle_nm[:-4]\n",
    "\n",
    "#     #             Append Data to the Dataframe\n",
    "#         df= df.append(a_series, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #     print(input_fle_nm)\n",
    "# # Delete the first 2 Rows of the Dataframe Because they are not Data\n",
    "# df = df.drop(index = [0, 1])\n",
    "# # Reset the Index so that it Starts with 0\n",
    "# df = df.reset_index(drop = True)\n",
    "# # Change the Data Types of Each Column\n",
    "# df = df.astype(convert_dict) \n",
    "# # Change the Date Column to a datetime Data Type\n",
    "# df['date']= pd.to_datetime(df['date'])\n",
    "    \n",
    "# avg_strmflw = df[\"streamflow_rate\"].mean(axis = 0)\n",
    "# print(f\"Average Streamflow: {avg_strmflw}\")\n",
    "# #     print(df[\"streamflow_rate\"])\n",
    "# #     print(\"********************************************************************\")\n",
    "# print (df)\n",
    "    \n",
    "# count = 0\n",
    "# print(len(df))\n",
    "\n",
    "# for i_df in range(len(df)):\n",
    "# #         print (df[\"streamflow_rate\"][2])\n",
    "#     print (df[\"streamflow_rate\"][i_df])\n",
    "# #         print (df_row)\n",
    "#     if df[\"streamflow_rate\"][i_df] < avg_strmflw:\n",
    "#         count = count + 1\n",
    "\n",
    "#         print (\"True\")\n",
    "#     elif df[\"streamflow_rate\"][i_df] > avg_strmflw:\n",
    "#         print (\"False\")\n",
    "#     print(\"********************************************************************\")\n",
    "\n",
    "# pct_blw_avg = (count / len(df) * 100)\n",
    "# print (pct_blw_avg)\n",
    "# # Add a value into an empty dictionay element\n",
    "# #     - https://www.pluralsight.com/guides/manipulating-lists-dictionaries-python\n",
    "# statn_data_lst_of_dicts[i].update({\"Data\": df})\n",
    "# statn_data_lst_of_dicts[i].update({\"Avg_Streamflow\": avg_strmflw})\n",
    "# statn_data_lst_of_dicts[i].update({\"Prcnt_Below_Avg\": pct_blw_avg})\n",
    "# #     input_fle_dict[\"Data\"].append(df)\n",
    "\n",
    "# # df_nm = df_nm.drop(index = [0, 1])\n",
    "# # \"_\" + statn_nm + \"_df\" = df\n",
    "# # df_nm\n",
    "# # df\n",
    "# # df.drop(index = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def find(name, path):\n",
    "for root, dirs, files in os.walk(input_fle_path):\n",
    "    if \"13073000.txt\" in files:\n",
    "        print (os.path.join(root, \"13073000.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all(\"13073000.txt\", input_fle_path):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(input_fle_path):\n",
    "        if name in files:\n",
    "            result.append(os.path.join(root, name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No County\n"
     ]
    }
   ],
   "source": [
    "# How to Ignore Hidden Files\n",
    "#     - https://stackoverflow.com/questions/15235823/how-to-ignore-hidden-files-in-python-functions\n",
    "\n",
    "# # *********************************************************************************************\n",
    "# # Create a List of the Files in the Directory\n",
    "# input_fle_dict = {\"Station_Nmbr\":[], \"File_Name\": [], \"df_Name\": [], \"Data\": [], \"Avg_Streamflow\": []}\n",
    "# # *********************************************************************************************\n",
    "\n",
    "# Create a List to Store/Save the Streamflow Data, Data will be Saved as a List of Dictionaries\n",
    "statn_data_lst_of_dicts = []\n",
    "input_fle_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "# # *********************************************************************************************\n",
    "# Put the Station in a list by County Name\n",
    "# First Find the station in the county  \n",
    "\n",
    "if statn_table_df_splt_StatnNmbr[\"text\"][1] == \"\":\n",
    "    print(\"No County\")\n",
    "# # *********************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "for input_fle_nm in os.listdir(input_fle_path):\n",
    "# Skip the Hidden Files in the Directory\n",
    "    if not input_fle_nm.startswith('.') and os.path.isfile(os.path.join(input_fle_path, input_fle_nm)):\n",
    "# # *********************************************************************************************\n",
    "# #         Append to the File Names to the Directory\n",
    "#         input_fle_dict[\"Station_Nmbr\"].append(input_fle_nm[:-4])\n",
    "#         input_fle_dict[\"File_Name\"].append(input_fle_nm)\n",
    "#         input_fle_dict[\"df_Name\"].append(\"_\" + input_fle_nm[:-4] + \"_df\")\n",
    "#         input_fle_dict[\"df_Name\"].append(\"_\" + input_fle_nm[:-4] + \"_df\")\n",
    "# #         print(input_fle_nm)\n",
    "# # *********************************************************************************************\n",
    "\n",
    "\n",
    "        input_fle_dict = {\"Station_Nmbr\": input_fle_nm[:-4], \n",
    "                          \"File_Name\": input_fle_nm, \n",
    "                          \"df_Name\": \"_\" + input_fle_nm[:-4] + \"_df\", \n",
    "                          \"Data\": \"\", \n",
    "                          \"Avg_Streamflow\": \"\",\n",
    "                          \"Prcnt_Below_Avg\": \"\"}\n",
    "\n",
    "# Append to the File Names to the Directory\n",
    "    statn_data_lst_of_dicts.append(dict(input_fle_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://note.nkmk.me/en/python-list-clear-pop-remove-del/#:~:text=In%20Python%2C%20use%20list%20methods,with%20an%20index%20or%20slice.\n",
    "\n",
    "statn_data_lst_of_dicts\n",
    "# del statn_data_lst_of_dicts[0]\n",
    "# statn_data_lst_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_data_lst_of_dicts[1]['Station_Nmbr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, station in enumerate(dict_nm[key]):\n",
    "for i, a_dict in enumerate(statn_data_lst_of_dicts):\n",
    "    print(statn_data_lst_of_dicts[i]['File_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = list(a_dict.keys())\n",
    "type(county)\n",
    "county[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Station_Nmbr': '12415070',\n",
       " 'File_Name': '12415070.txt',\n",
       " 'df_Name': '_12415070_df',\n",
       " 'Data': '',\n",
       " 'Avg_Streamflow': '',\n",
       " 'Prcnt_Below_Avg': ''}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_series = pd.Series(data = [0,1,2,3,4], index = [\"agency\", \"site_nmbr\", \"date\", \"streamflow_rate\", \"approved/pending\"])\n",
    "a_series\n",
    "statn_data_lst_of_dicts\n",
    "a_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index No: 0\n",
      "County: Ada County\n",
      "Stations:13206000.txt\n",
      "Average Streamflow: 175.51612903225808\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  13206000 1990-01-01            183.0                A\n",
      "1    USGS  13206000 1990-01-02            185.0                A\n",
      "2    USGS  13206000 1990-01-03            184.0                A\n",
      "3    USGS  13206000 1990-01-04            189.0                A\n",
      "4    USGS  13206000 1990-01-05            182.0                A\n",
      "5    USGS  13206000 1990-01-06            182.0                A\n",
      "6    USGS  13206000 1990-01-07            209.0                A\n",
      "7    USGS  13206000 1990-01-08            201.0                A\n",
      "8    USGS  13206000 1990-01-09            189.0                A\n",
      "9    USGS  13206000 1990-01-10            186.0                A\n",
      "10   USGS  13206000 1990-01-11            176.0                A\n",
      "11   USGS  13206000 1990-01-12            170.0                A\n",
      "12   USGS  13206000 1990-01-13            169.0                A\n",
      "13   USGS  13206000 1990-01-14            169.0                A\n",
      "14   USGS  13206000 1990-01-15            165.0                A\n",
      "15   USGS  13206000 1990-01-16            167.0                A\n",
      "16   USGS  13206000 1990-01-17            166.0                A\n",
      "17   USGS  13206000 1990-01-18            169.0                A\n",
      "18   USGS  13206000 1990-01-19            168.0                A\n",
      "19   USGS  13206000 1990-01-20            168.0                A\n",
      "20   USGS  13206000 1990-01-21            167.0                A\n",
      "21   USGS  13206000 1990-01-22            168.0                A\n",
      "22   USGS  13206000 1990-01-23            167.0                A\n",
      "23   USGS  13206000 1990-01-24            164.0                A\n",
      "24   USGS  13206000 1990-01-25            166.0                A\n",
      "25   USGS  13206000 1990-01-26            166.0                A\n",
      "26   USGS  13206000 1990-01-27            172.0                A\n",
      "27   USGS  13206000 1990-01-28            174.0                A\n",
      "28   USGS  13206000 1990-01-29            172.0                A\n",
      "29   USGS  13206000 1990-01-30            176.0                A\n",
      "30   USGS  13206000 1990-01-31            172.0                A\n",
      "----------------------------------------------------\n",
      "Stations:13206305.txt\n",
      "Stations:13206400.txt\n",
      "********************************************************************\n",
      "Index No: 1\n",
      "County: Bannock County\n",
      "Stations:13073000.txt\n",
      "Average Streamflow: 138.38709677419354\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  13073000 1990-01-01            133.0                A\n",
      "1    USGS  13073000 1990-01-02            136.0                A\n",
      "2    USGS  13073000 1990-01-03            133.0                A\n",
      "3    USGS  13073000 1990-01-04            135.0                A\n",
      "4    USGS  13073000 1990-01-05            134.0                A\n",
      "5    USGS  13073000 1990-01-06            134.0                A\n",
      "6    USGS  13073000 1990-01-07            132.0                A\n",
      "7    USGS  13073000 1990-01-08            164.0                A\n",
      "8    USGS  13073000 1990-01-09            146.0                A\n",
      "9    USGS  13073000 1990-01-10            144.0                A\n",
      "10   USGS  13073000 1990-01-11            143.0                A\n",
      "11   USGS  13073000 1990-01-12            143.0                A\n",
      "12   USGS  13073000 1990-01-13            143.0                A\n",
      "13   USGS  13073000 1990-01-14            147.0                A\n",
      "14   USGS  13073000 1990-01-15            146.0                A\n",
      "15   USGS  13073000 1990-01-16            144.0                A\n",
      "16   USGS  13073000 1990-01-17            142.0                A\n",
      "17   USGS  13073000 1990-01-18            137.0                A\n",
      "18   USGS  13073000 1990-01-19            134.0                A\n",
      "19   USGS  13073000 1990-01-20            134.0                A\n",
      "20   USGS  13073000 1990-01-21            134.0                A\n",
      "21   USGS  13073000 1990-01-22            134.0                A\n",
      "22   USGS  13073000 1990-01-23            135.0                A\n",
      "23   USGS  13073000 1990-01-24            135.0                A\n",
      "24   USGS  13073000 1990-01-25            134.0                A\n",
      "25   USGS  13073000 1990-01-26            135.0                A\n",
      "26   USGS  13073000 1990-01-27            135.0                A\n",
      "27   USGS  13073000 1990-01-28            135.0                A\n",
      "28   USGS  13073000 1990-01-29            136.0                A\n",
      "29   USGS  13073000 1990-01-30            137.0                A\n",
      "30   USGS  13073000 1990-01-31            136.0                A\n",
      "----------------------------------------------------\n",
      "Stations:13075000.txt\n",
      "Average Streamflow: 78.38709677419355\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  13075000 1990-01-01             68.0                A\n",
      "1    USGS  13075000 1990-01-02             70.0                A\n",
      "2    USGS  13075000 1990-01-03             66.0                A\n",
      "3    USGS  13075000 1990-01-04             72.0                A\n",
      "4    USGS  13075000 1990-01-05             71.0                A\n",
      "5    USGS  13075000 1990-01-06             71.0                A\n",
      "6    USGS  13075000 1990-01-07             69.0                A\n",
      "7    USGS  13075000 1990-01-08            121.0                A\n",
      "8    USGS  13075000 1990-01-09            107.0                A\n",
      "9    USGS  13075000 1990-01-10             89.0                A\n",
      "10   USGS  13075000 1990-01-11             87.0                A\n",
      "11   USGS  13075000 1990-01-12             82.0                A\n",
      "12   USGS  13075000 1990-01-13             81.0                A\n",
      "13   USGS  13075000 1990-01-14             88.0                A\n",
      "14   USGS  13075000 1990-01-15             92.0                A\n",
      "15   USGS  13075000 1990-01-16             88.0                A\n",
      "16   USGS  13075000 1990-01-17             85.0                A\n",
      "17   USGS  13075000 1990-01-18             81.0                A\n",
      "18   USGS  13075000 1990-01-19             78.0                A\n",
      "19   USGS  13075000 1990-01-20             76.0                A\n",
      "20   USGS  13075000 1990-01-21             73.0                A\n",
      "21   USGS  13075000 1990-01-22             73.0                A\n",
      "22   USGS  13075000 1990-01-23             73.0                A\n",
      "23   USGS  13075000 1990-01-24             68.0                A\n",
      "24   USGS  13075000 1990-01-25             74.0                A\n",
      "25   USGS  13075000 1990-01-26             72.0                A\n",
      "26   USGS  13075000 1990-01-27             65.0                A\n",
      "27   USGS  13075000 1990-01-28             74.0                A\n",
      "28   USGS  13075000 1990-01-29             72.0                A\n",
      "29   USGS  13075000 1990-01-30             72.0                A\n",
      "30   USGS  13075000 1990-01-31             72.0                A\n",
      "----------------------------------------------------\n",
      "Stations:13075500.txt\n",
      "Average Streamflow: 241.03225806451613\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  13075500 1990-01-01            221.0                A\n",
      "1    USGS  13075500 1990-01-02            225.0                A\n",
      "2    USGS  13075500 1990-01-03            226.0                A\n",
      "3    USGS  13075500 1990-01-04            221.0                A\n",
      "4    USGS  13075500 1990-01-05            225.0                A\n",
      "5    USGS  13075500 1990-01-06            223.0                A\n",
      "6    USGS  13075500 1990-01-07            223.0                A\n",
      "7    USGS  13075500 1990-01-08            245.0                A\n",
      "8    USGS  13075500 1990-01-09            343.0                A\n",
      "9    USGS  13075500 1990-01-10            291.0                A\n",
      "10   USGS  13075500 1990-01-11            263.0                A\n",
      "11   USGS  13075500 1990-01-12            257.0                A\n",
      "12   USGS  13075500 1990-01-13            249.0                A\n",
      "13   USGS  13075500 1990-01-14            256.0                A\n",
      "14   USGS  13075500 1990-01-15            270.0                A\n",
      "15   USGS  13075500 1990-01-16            262.0                A\n",
      "16   USGS  13075500 1990-01-17            253.0                A\n",
      "17   USGS  13075500 1990-01-18            244.0                A\n",
      "18   USGS  13075500 1990-01-19            234.0                A\n",
      "19   USGS  13075500 1990-01-20            230.0                A\n",
      "20   USGS  13075500 1990-01-21            229.0                A\n",
      "21   USGS  13075500 1990-01-22            230.0                A\n",
      "22   USGS  13075500 1990-01-23            230.0                A\n",
      "23   USGS  13075500 1990-01-24            230.0                A\n",
      "24   USGS  13075500 1990-01-25            225.0                A\n",
      "25   USGS  13075500 1990-01-26            229.0                A\n",
      "26   USGS  13075500 1990-01-27            227.0                A\n",
      "27   USGS  13075500 1990-01-28            220.0              A:e\n",
      "28   USGS  13075500 1990-01-29            225.0              A:e\n",
      "29   USGS  13075500 1990-01-30            232.0                A\n",
      "30   USGS  13075500 1990-01-31            234.0                A\n",
      "----------------------------------------------------\n",
      "Stations:13075910.txt\n",
      "Average Streamflow: 492.5483870967742\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  13075910 1990-01-01            470.0              A:e\n",
      "1    USGS  13075910 1990-01-02            470.0              A:e\n",
      "2    USGS  13075910 1990-01-03            475.0              A:e\n",
      "3    USGS  13075910 1990-01-04            470.0              A:e\n",
      "4    USGS  13075910 1990-01-05            465.0              A:e\n",
      "5    USGS  13075910 1990-01-06            465.0              A:e\n",
      "6    USGS  13075910 1990-01-07            470.0              A:e\n",
      "7    USGS  13075910 1990-01-08            500.0              A:e\n",
      "8    USGS  13075910 1990-01-09            590.0              A:e\n",
      "9    USGS  13075910 1990-01-10            550.0              A:e\n",
      "10   USGS  13075910 1990-01-11            520.0              A:e\n",
      "11   USGS  13075910 1990-01-12            515.0              A:e\n",
      "12   USGS  13075910 1990-01-13            520.0              A:e\n",
      "13   USGS  13075910 1990-01-14            525.0              A:e\n",
      "14   USGS  13075910 1990-01-15            535.0              A:e\n",
      "15   USGS  13075910 1990-01-16            525.0              A:e\n",
      "16   USGS  13075910 1990-01-17            515.0              A:e\n",
      "17   USGS  13075910 1990-01-18            500.0              A:e\n",
      "18   USGS  13075910 1990-01-19            480.0              A:e\n",
      "19   USGS  13075910 1990-01-20            475.0              A:e\n",
      "20   USGS  13075910 1990-01-21            470.0              A:e\n",
      "21   USGS  13075910 1990-01-22            470.0              A:e\n",
      "22   USGS  13075910 1990-01-23            470.0              A:e\n",
      "23   USGS  13075910 1990-01-24            470.0              A:e\n",
      "24   USGS  13075910 1990-01-25            483.0                A\n",
      "25   USGS  13075910 1990-01-26            486.0                A\n",
      "26   USGS  13075910 1990-01-27            483.0                A\n",
      "27   USGS  13075910 1990-01-28            480.0                A\n",
      "28   USGS  13075910 1990-01-29            476.0                A\n",
      "29   USGS  13075910 1990-01-30            473.0                A\n",
      "30   USGS  13075910 1990-01-31            473.0                A\n",
      "----------------------------------------------------\n",
      "********************************************************************\n",
      "Index No: 2\n",
      "County: Bear Lake County\n",
      "Stations:10039500.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Streamflow: 142.16129032258064\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  10039500 1990-01-01            143.0              A:e\n",
      "1    USGS  10039500 1990-01-02            147.0              A:e\n",
      "2    USGS  10039500 1990-01-03            141.0              A:e\n",
      "3    USGS  10039500 1990-01-04            140.0              A:e\n",
      "4    USGS  10039500 1990-01-05            143.0              A:e\n",
      "5    USGS  10039500 1990-01-06            140.0              A:e\n",
      "6    USGS  10039500 1990-01-07            138.0              A:e\n",
      "7    USGS  10039500 1990-01-08            141.0              A:e\n",
      "8    USGS  10039500 1990-01-09            139.0              A:e\n",
      "9    USGS  10039500 1990-01-10            140.0              A:e\n",
      "10   USGS  10039500 1990-01-11            142.0              A:e\n",
      "11   USGS  10039500 1990-01-12            150.0              A:e\n",
      "12   USGS  10039500 1990-01-13            158.0              A:e\n",
      "13   USGS  10039500 1990-01-14            160.0              A:e\n",
      "14   USGS  10039500 1990-01-15            154.0              A:e\n",
      "15   USGS  10039500 1990-01-16            154.0              A:e\n",
      "16   USGS  10039500 1990-01-17            150.0              A:e\n",
      "17   USGS  10039500 1990-01-18            141.0              A:e\n",
      "18   USGS  10039500 1990-01-19            134.0              A:e\n",
      "19   USGS  10039500 1990-01-20            141.0              A:e\n",
      "20   USGS  10039500 1990-01-21            150.0              A:e\n",
      "21   USGS  10039500 1990-01-22            141.0              A:e\n",
      "22   USGS  10039500 1990-01-23            132.0              A:e\n",
      "23   USGS  10039500 1990-01-24            141.0              A:e\n",
      "24   USGS  10039500 1990-01-25            132.0              A:e\n",
      "25   USGS  10039500 1990-01-26            141.0              A:e\n",
      "26   USGS  10039500 1990-01-27            148.0              A:e\n",
      "27   USGS  10039500 1990-01-28            137.0              A:e\n",
      "28   USGS  10039500 1990-01-29            138.0              A:e\n",
      "29   USGS  10039500 1990-01-30            126.0              A:e\n",
      "30   USGS  10039500 1990-01-31            125.0              A:e\n",
      "----------------------------------------------------\n",
      "Stations:10068500.txt\n",
      "Average Streamflow: 82.3225806451613\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  10068500 1990-01-01             88.0              A:e\n",
      "1    USGS  10068500 1990-01-02             86.0              A:e\n",
      "2    USGS  10068500 1990-01-03             86.0              A:e\n",
      "3    USGS  10068500 1990-01-04             85.0              A:e\n",
      "4    USGS  10068500 1990-01-05             85.0              A:e\n",
      "5    USGS  10068500 1990-01-06             86.0              A:e\n",
      "6    USGS  10068500 1990-01-07             88.0              A:e\n",
      "7    USGS  10068500 1990-01-08             88.0              A:e\n",
      "8    USGS  10068500 1990-01-09             90.0              A:e\n",
      "9    USGS  10068500 1990-01-10             87.0              A:e\n",
      "10   USGS  10068500 1990-01-11             86.0              A:e\n",
      "11   USGS  10068500 1990-01-12             84.0              A:e\n",
      "12   USGS  10068500 1990-01-13             84.0              A:e\n",
      "13   USGS  10068500 1990-01-14             82.0              A:e\n",
      "14   USGS  10068500 1990-01-15             82.0              A:e\n",
      "15   USGS  10068500 1990-01-16             80.0              A:e\n",
      "16   USGS  10068500 1990-01-17             82.0              A:e\n",
      "17   USGS  10068500 1990-01-18             79.0              A:e\n",
      "18   USGS  10068500 1990-01-19             78.0              A:e\n",
      "19   USGS  10068500 1990-01-20             75.0              A:e\n",
      "20   USGS  10068500 1990-01-21             78.0              A:e\n",
      "21   USGS  10068500 1990-01-22             81.0              A:e\n",
      "22   USGS  10068500 1990-01-23             78.0              A:e\n",
      "23   USGS  10068500 1990-01-24             81.0              A:e\n",
      "24   USGS  10068500 1990-01-25             78.0              A:e\n",
      "25   USGS  10068500 1990-01-26             78.0              A:e\n",
      "26   USGS  10068500 1990-01-27             80.0              A:e\n",
      "27   USGS  10068500 1990-01-28             80.0              A:e\n",
      "28   USGS  10068500 1990-01-29             78.0              A:e\n",
      "29   USGS  10068500 1990-01-30             81.0              A:e\n",
      "30   USGS  10068500 1990-01-31             78.0              A:e\n",
      "----------------------------------------------------\n",
      "********************************************************************\n",
      "Index No: 3\n",
      "County: Benewah County\n",
      "Stations:12414900.txt\n",
      "Average Streamflow: 489.4516129032258\n",
      "Data:   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  12414900 1990-01-01             74.0              A:e\n",
      "1    USGS  12414900 1990-01-02             74.0              A:e\n",
      "2    USGS  12414900 1990-01-03             68.0                A\n",
      "3    USGS  12414900 1990-01-04             95.0                A\n",
      "4    USGS  12414900 1990-01-05            102.0                A\n",
      "5    USGS  12414900 1990-01-06            211.0                A\n",
      "6    USGS  12414900 1990-01-07           1250.0                A\n",
      "7    USGS  12414900 1990-01-08           2520.0                A\n",
      "8    USGS  12414900 1990-01-09           1560.0                A\n",
      "9    USGS  12414900 1990-01-10           2100.0                A\n",
      "10   USGS  12414900 1990-01-11           1020.0                A\n",
      "11   USGS  12414900 1990-01-12            672.0                A\n",
      "12   USGS  12414900 1990-01-13            515.0                A\n",
      "13   USGS  12414900 1990-01-14            567.0                A\n",
      "14   USGS  12414900 1990-01-15            543.0                A\n",
      "15   USGS  12414900 1990-01-16            453.0                A\n",
      "16   USGS  12414900 1990-01-17            391.0                A\n",
      "17   USGS  12414900 1990-01-18            333.0                A\n",
      "18   USGS  12414900 1990-01-19            260.0                A\n",
      "19   USGS  12414900 1990-01-20            240.0              A:e\n",
      "20   USGS  12414900 1990-01-21            230.0              A:e\n",
      "21   USGS  12414900 1990-01-22            238.0                A\n",
      "22   USGS  12414900 1990-01-23            221.0                A\n",
      "23   USGS  12414900 1990-01-24            199.0                A\n",
      "24   USGS  12414900 1990-01-25            187.0                A\n",
      "25   USGS  12414900 1990-01-26            196.0                A\n",
      "26   USGS  12414900 1990-01-27            172.0                A\n",
      "27   USGS  12414900 1990-01-28            172.0                A\n",
      "28   USGS  12414900 1990-01-29            171.0                A\n",
      "29   USGS  12414900 1990-01-30            173.0                A\n",
      "30   USGS  12414900 1990-01-31            166.0                A\n",
      "----------------------------------------------------\n",
      "Stations:12415070.txt\n",
      "Stations:12415135.txt\n",
      "********************************************************************\n",
      "Index No: 4\n",
      "County: Bingham County\n",
      "Stations:13060000.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/Idaho_Streamflow_Data/13060000.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-a3de53e2ca17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# *********************************************************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Loop Through the .txt File and Store the Data into a Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fle_path_fr_lp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;31m#             print(fh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#             print(\"----------------------------------------------------\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/Idaho_Streamflow_Data/13060000.txt'"
     ]
    }
   ],
   "source": [
    "# References:\n",
    "#     - Break a Loop:\n",
    "#         - https://www.programiz.com/python-programming/break-continue\n",
    "\n",
    "# for i, station in enumerate(dict_nm[key]):\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                           Step 1 Loop Through Each County \n",
    "# *********************************************************************************************\n",
    "\n",
    "statns_wth_no_strmflw_data = []\n",
    "\n",
    "for i, county_dict in enumerate(cnty_lst):\n",
    "    print(f\"Index No: {i}\")\n",
    "# Get dictionary keys as a list\n",
    "# https://www.geeksforgeeks.org/python-get-dictionary-keys-as-a-list/\n",
    "    county_lst = list(county_dict.keys())\n",
    "    county = county_lst[0]\n",
    "    print(f\"County: {county}\")\n",
    "#     print(f\"Stations:\")\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                    Step 2 Loop Through Each Station Within a County\n",
    "# *********************************************************************************************\n",
    "# \n",
    "    for station_dict in county_dict[county]:\n",
    "        station_lst_keys = list(station_dict.keys())\n",
    "        station_lst_File_Name = station_lst_keys[1]\n",
    "        station_lst_data = station_lst_keys[3]\n",
    "#         print(station_lst_keys)\n",
    "#         print(station_lst_File_Name)\n",
    "#         print(station_lst)\n",
    "        print(f\"Stations:{station_dict[station_lst_File_Name]}\")\n",
    "#     pprint.pprint(a_dict[county])\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                   Step 3 - Create the File Path for the Stream Data\n",
    "# *********************************************************************************************\n",
    "\n",
    "# Create the File Path for Each Station's .txt File Which Includes Streamflow Data\n",
    "        input_fle_path_fr_lp = input_fle_path + \"/\" + station_dict[station_lst_File_Name]\n",
    "#         print(f\"File Path: {input_fle_path_fr_lp}\")\n",
    "#         print(\"----------------------------------------------------\")\n",
    "        \n",
    "# Create the Dataframe to Store the Streamflow Data from the .txt File\n",
    "        df = pd.DataFrame(columns = clmn_nms)\n",
    "\n",
    "# *********************************************************************************************\n",
    "#             Step 4 - Loop Through the .txt File and Store the Data into a Dataframe\n",
    "# *********************************************************************************************\n",
    "# Loop Through the .txt File and Store the Data into a Dataframe\n",
    "        with open(input_fle_path_fr_lp,'r') as fh:\n",
    "#             print(fh)\n",
    "#             print(\"----------------------------------------------------\")\n",
    "            for curline in dropwhile(is_comment, fh):\n",
    "#                 print(f\"Curline:\")\n",
    "#                 print(type(curline))\n",
    "#                 print(\"----------------------------------------------------\")\n",
    "    #         print(f\"Index Number: {count} {curline}\")\n",
    "    #         count = count + 1\n",
    "\n",
    "    # Split a String\n",
    "    #     - https://www.geeksforgeeks.org/python-string-split/\n",
    "    # Pandas Series\n",
    "    #     - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "                to_append = curline[:-1].split(\"\\t\")\n",
    "#                 print(f\"To Append:\")\n",
    "#                 print(to_append)\n",
    "# #                 print(to_append[4])\n",
    "#                 print(\"----------------------------------------------------\")\n",
    "                \n",
    "                if len(to_append) == 5:\n",
    "#                     print(\"True\")\n",
    "            \n",
    "                    a_series = pd.Series(data = to_append, index = clmn_nms)\n",
    "#                     print(clmn_nms)\n",
    "#                     print(\"Series:\")\n",
    "#                     print(a_series)\n",
    "#                     print(\"----------------------------------------------------\")\n",
    "                    \n",
    "                        #             Dataframe\n",
    "    #             Dataframe Name\n",
    "                    statn_nm = input_fle_nm[:-4]\n",
    "\n",
    "    #             Append Data to the Dataframe\n",
    "                    df= df.append(a_series, ignore_index=True)\n",
    "\n",
    "                    \n",
    "            \n",
    "                elif len(to_append) < 5:\n",
    "                    statns_wth_no_strmflw_data.append(station_dict[station_lst_File_Name])\n",
    "                    break\n",
    "#                     print(f\"Station {station_dict[station_lst_File_Name]} Does not \")\n",
    "                    \n",
    "#     #             Dataframe\n",
    "#     #             Dataframe Name\n",
    "#                 statn_nm = input_fle_nm[:-4]\n",
    "\n",
    "        if len(df) != 0:\n",
    "# Delete the first 2 Rows of the Dataframe Because they are not Data\n",
    "            df = df.drop(index = [0, 1])\n",
    "# Reset the Index so that it Starts with 0\n",
    "            df = df.reset_index(drop = True)\n",
    "# Change the Data Types of Each Column\n",
    "            df = df.astype(convert_dict) \n",
    "# Change the Date Column to a datetime Data Type\n",
    "            df['date']= pd.to_datetime(df['date'])\n",
    "\n",
    "# Calculate the Averge Streamflow Rate\n",
    "            avg_strmflw = df[\"streamflow_rate\"].mean(axis = 0)\n",
    "#     avg_strmflw = statn_table_df_splt_StatnNmbr_test[\"extndd_yrs_mean\"]\n",
    "            print(f\"Average Streamflow: {avg_strmflw}\")\n",
    "    \n",
    "#     #             Append Data to the Dataframe           \n",
    "#             print(df)\n",
    "            station_dict[station_lst_data] = df\n",
    "            print(f\"Data:{station_dict[station_lst_data]}\")\n",
    "            print(\"----------------------------------------------------\")\n",
    "            \n",
    "    print(\"********************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13206305.txt', '13206400.txt', '12415070.txt', '12415135.txt']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statns_wth_no_strmflw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_data_lst[\"Station_Nmbr\"][0]\n",
    "# input_fle_dict[\"File_Name\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for station in input_fle_dict[\"df_Name\"]:\n",
    "#     print(station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_nm = input_fle_lst[0]\n",
    "input_fle_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40482738/how-to-name-dataframe-with-variables-in-pandas\n",
    "\n",
    "N = 10 # 5 in sample\n",
    "dfs = {'name' + str(i):df for i in range(1,N)}\n",
    "print (dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[\"name2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 # 5 in sample\n",
    "# for input_fle_nm in input_fle_lst:\n",
    "input_fle_nm =\"\"\n",
    "dfs = {input_fle_nm:df for input_fle_nm in input_fle_dict[\"df_Name\"]}\n",
    "# print (input_fle_nm)\n",
    "print (dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_data_lst_of_dicts[2]['File_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agency                          agency_cd\n",
      "site_nmbr                         site_no\n",
      "date                             datetime\n",
      "streamflow_rate        143401_00060_00003\n",
      "approved/pending    143401_00060_00003_cd\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency               5s\n",
      "site_nmbr           15s\n",
      "date                20d\n",
      "streamflow_rate     14n\n",
      "approved/pending    10s\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-01\n",
      "streamflow_rate            143\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-02\n",
      "streamflow_rate            147\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-03\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-04\n",
      "streamflow_rate            140\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-05\n",
      "streamflow_rate            143\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-06\n",
      "streamflow_rate            140\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-07\n",
      "streamflow_rate            138\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-08\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-09\n",
      "streamflow_rate            139\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-10\n",
      "streamflow_rate            140\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-11\n",
      "streamflow_rate            142\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-12\n",
      "streamflow_rate            150\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-13\n",
      "streamflow_rate            158\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-14\n",
      "streamflow_rate            160\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-15\n",
      "streamflow_rate            154\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-16\n",
      "streamflow_rate            154\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-17\n",
      "streamflow_rate            150\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-18\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-19\n",
      "streamflow_rate            134\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-20\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-21\n",
      "streamflow_rate            150\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-22\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-23\n",
      "streamflow_rate            132\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-24\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-25\n",
      "streamflow_rate            132\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-26\n",
      "streamflow_rate            141\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-27\n",
      "streamflow_rate            148\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-28\n",
      "streamflow_rate            137\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-29\n",
      "streamflow_rate            138\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-30\n",
      "streamflow_rate            126\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             10039500\n",
      "date                1990-01-31\n",
      "streamflow_rate            125\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "Average Streamflow: 142.16129032258064\n",
      "   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  10039500 1990-01-01            143.0              A:e\n",
      "1    USGS  10039500 1990-01-02            147.0              A:e\n",
      "2    USGS  10039500 1990-01-03            141.0              A:e\n",
      "3    USGS  10039500 1990-01-04            140.0              A:e\n",
      "4    USGS  10039500 1990-01-05            143.0              A:e\n",
      "5    USGS  10039500 1990-01-06            140.0              A:e\n",
      "6    USGS  10039500 1990-01-07            138.0              A:e\n",
      "7    USGS  10039500 1990-01-08            141.0              A:e\n",
      "8    USGS  10039500 1990-01-09            139.0              A:e\n",
      "9    USGS  10039500 1990-01-10            140.0              A:e\n",
      "10   USGS  10039500 1990-01-11            142.0              A:e\n",
      "11   USGS  10039500 1990-01-12            150.0              A:e\n",
      "12   USGS  10039500 1990-01-13            158.0              A:e\n",
      "13   USGS  10039500 1990-01-14            160.0              A:e\n",
      "14   USGS  10039500 1990-01-15            154.0              A:e\n",
      "15   USGS  10039500 1990-01-16            154.0              A:e\n",
      "16   USGS  10039500 1990-01-17            150.0              A:e\n",
      "17   USGS  10039500 1990-01-18            141.0              A:e\n",
      "18   USGS  10039500 1990-01-19            134.0              A:e\n",
      "19   USGS  10039500 1990-01-20            141.0              A:e\n",
      "20   USGS  10039500 1990-01-21            150.0              A:e\n",
      "21   USGS  10039500 1990-01-22            141.0              A:e\n",
      "22   USGS  10039500 1990-01-23            132.0              A:e\n",
      "23   USGS  10039500 1990-01-24            141.0              A:e\n",
      "24   USGS  10039500 1990-01-25            132.0              A:e\n",
      "25   USGS  10039500 1990-01-26            141.0              A:e\n",
      "26   USGS  10039500 1990-01-27            148.0              A:e\n",
      "27   USGS  10039500 1990-01-28            137.0              A:e\n",
      "28   USGS  10039500 1990-01-29            138.0              A:e\n",
      "29   USGS  10039500 1990-01-30            126.0              A:e\n",
      "30   USGS  10039500 1990-01-31            125.0              A:e\n",
      "31\n",
      "143.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "147.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "140.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "143.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "140.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "138.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "139.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "140.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "142.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "150.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "158.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "160.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "154.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "154.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "150.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "134.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "150.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "132.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "132.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "141.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "148.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "137.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "138.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "126.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "125.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "64.51612903225806\n",
      "agency                         agency_cd\n",
      "site_nmbr                        site_no\n",
      "date                            datetime\n",
      "streamflow_rate        45404_00060_00003\n",
      "approved/pending    45404_00060_00003_cd\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency               5s\n",
      "site_nmbr           15s\n",
      "date                20d\n",
      "streamflow_rate     14n\n",
      "approved/pending    10s\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-01\n",
      "streamflow_rate            221\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-02\n",
      "streamflow_rate            225\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-03\n",
      "streamflow_rate            226\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-04\n",
      "streamflow_rate            221\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-05\n",
      "streamflow_rate            225\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-06\n",
      "streamflow_rate            223\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-07\n",
      "streamflow_rate            223\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-08\n",
      "streamflow_rate            245\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-09\n",
      "streamflow_rate            343\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-10\n",
      "streamflow_rate            291\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-11\n",
      "streamflow_rate            263\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-12\n",
      "streamflow_rate            257\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-13\n",
      "streamflow_rate            249\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-14\n",
      "streamflow_rate            256\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-15\n",
      "streamflow_rate            270\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-16\n",
      "streamflow_rate            262\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-17\n",
      "streamflow_rate            253\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-18\n",
      "streamflow_rate            244\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-19\n",
      "streamflow_rate            234\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-20\n",
      "streamflow_rate            230\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-21\n",
      "streamflow_rate            229\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-22\n",
      "streamflow_rate            230\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-23\n",
      "streamflow_rate            230\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-24\n",
      "streamflow_rate            230\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-25\n",
      "streamflow_rate            225\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-26\n",
      "streamflow_rate            229\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-27\n",
      "streamflow_rate            227\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-28\n",
      "streamflow_rate            220\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-29\n",
      "streamflow_rate            225\n",
      "approved/pending           A:e\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-30\n",
      "streamflow_rate            232\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "agency                    USGS\n",
      "site_nmbr             13075500\n",
      "date                1990-01-31\n",
      "streamflow_rate            234\n",
      "approved/pending             A\n",
      "dtype: object\n",
      "********************************************************************\n",
      "Average Streamflow: 241.03225806451613\n",
      "   agency site_nmbr       date  streamflow_rate approved/pending\n",
      "0    USGS  13075500 1990-01-01            221.0                A\n",
      "1    USGS  13075500 1990-01-02            225.0                A\n",
      "2    USGS  13075500 1990-01-03            226.0                A\n",
      "3    USGS  13075500 1990-01-04            221.0                A\n",
      "4    USGS  13075500 1990-01-05            225.0                A\n",
      "5    USGS  13075500 1990-01-06            223.0                A\n",
      "6    USGS  13075500 1990-01-07            223.0                A\n",
      "7    USGS  13075500 1990-01-08            245.0                A\n",
      "8    USGS  13075500 1990-01-09            343.0                A\n",
      "9    USGS  13075500 1990-01-10            291.0                A\n",
      "10   USGS  13075500 1990-01-11            263.0                A\n",
      "11   USGS  13075500 1990-01-12            257.0                A\n",
      "12   USGS  13075500 1990-01-13            249.0                A\n",
      "13   USGS  13075500 1990-01-14            256.0                A\n",
      "14   USGS  13075500 1990-01-15            270.0                A\n",
      "15   USGS  13075500 1990-01-16            262.0                A\n",
      "16   USGS  13075500 1990-01-17            253.0                A\n",
      "17   USGS  13075500 1990-01-18            244.0                A\n",
      "18   USGS  13075500 1990-01-19            234.0                A\n",
      "19   USGS  13075500 1990-01-20            230.0                A\n",
      "20   USGS  13075500 1990-01-21            229.0                A\n",
      "21   USGS  13075500 1990-01-22            230.0                A\n",
      "22   USGS  13075500 1990-01-23            230.0                A\n",
      "23   USGS  13075500 1990-01-24            230.0                A\n",
      "24   USGS  13075500 1990-01-25            225.0                A\n",
      "25   USGS  13075500 1990-01-26            229.0                A\n",
      "26   USGS  13075500 1990-01-27            227.0                A\n",
      "27   USGS  13075500 1990-01-28            220.0              A:e\n",
      "28   USGS  13075500 1990-01-29            225.0              A:e\n",
      "29   USGS  13075500 1990-01-30            232.0                A\n",
      "30   USGS  13075500 1990-01-31            234.0                A\n",
      "31\n",
      "221.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "225.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "226.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "221.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "225.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "223.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "223.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "245.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "343.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "291.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "263.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "257.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "249.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "256.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "270.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "262.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "253.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "244.0\n",
      "False\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "234.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "230.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "229.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "230.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "230.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "230.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "225.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "229.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "227.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "220.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "225.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "232.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "234.0\n",
      "True\n",
      "********************************************************************\n",
      "********************************************************************\n",
      "64.51612903225806\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 3, index implies 5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-fb295fba29aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#     - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mto_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0ma_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_append\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclmn_nms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_series\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"********************************************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                         raise ValueError(\n\u001b[0;32m--> 322\u001b[0;31m                             \u001b[0;34mf\"Length of passed values is {len(data)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                             \u001b[0;34mf\"index implies {len(index)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                         )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of passed values is 3, index implies 5."
     ]
    }
   ],
   "source": [
    "for i, a_dict in enumerate(statn_data_lst_of_dicts):\n",
    "# *********************************************************************************************\n",
    "#                                   Step 1 Create Variables\n",
    "# *********************************************************************************************\n",
    "\n",
    "#     Create the File Path for Each Station's .txt File Which Includes Streamflow Data\n",
    "    input_fle_path_fr_lp = input_fle_path + \"/\" + statn_data_lst_of_dicts[i]['File_Name']\n",
    "\n",
    "#     Create the Dataframe to Store the Streamflow Data from the .txt File\n",
    "    df = pd.DataFrame(columns = clmn_nms)\n",
    "#     print(df)\n",
    "#     df_nm = \"_\" + statn_nm + \"_df\"\n",
    "# *********************************************************************************************\n",
    "\n",
    "# *********************************************************************************************\n",
    "#                           Step 2 Create a Dataframe for the Streamflow\n",
    "# *********************************************************************************************\n",
    "# Loop Through the .txt File and Store the Data into a Dataframe\n",
    "    with open(input_fle_path_fr_lp,'r') as fh:\n",
    "        for curline in dropwhile(is_comment, fh):\n",
    "#             print(curline)\n",
    "    #         print(f\"Index Number: {count} {curline}\")\n",
    "    #         count = count + 1\n",
    "\n",
    "\n",
    "\n",
    "    # Split a String\n",
    "    #     - https://www.geeksforgeeks.org/python-string-split/\n",
    "    # Pandas Series\n",
    "    #     - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "            to_append = curline[:-1].split(\"\\t\")\n",
    "            a_series = pd.Series(to_append, index = clmn_nms)\n",
    "            print(a_series)\n",
    "            print(\"********************************************************************\")\n",
    "    #             Dataframe\n",
    "    #             Dataframe Name\n",
    "            statn_nm = input_fle_nm[:-4]\n",
    "\n",
    "    #             Append Data to the Dataframe\n",
    "            df= df.append(a_series, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(input_fle_nm)\n",
    "# Delete the first 2 Rows of the Dataframe Because they are not Data\n",
    "    df = df.drop(index = [0, 1])\n",
    "# Reset the Index so that it Starts with 0\n",
    "    df = df.reset_index(drop = True)\n",
    "# Change the Data Types of Each Column\n",
    "    df = df.astype(convert_dict) \n",
    "# Change the Date Column to a datetime Data Type\n",
    "    df['date']= pd.to_datetime(df['date'])\n",
    "    \n",
    "    avg_strmflw = df[\"streamflow_rate\"].mean(axis = 0)\n",
    "#     avg_strmflw = statn_table_df_splt_StatnNmbr_test[\"extndd_yrs_mean\"]\n",
    "    print(f\"Average Streamflow: {avg_strmflw}\")\n",
    "#     print(df[\"streamflow_rate\"])\n",
    "#     print(\"********************************************************************\")\n",
    "    print (df)\n",
    "    \n",
    "    count = 0\n",
    "    print(len(df))\n",
    "    \n",
    "    for i_df in range(len(df)):\n",
    "#         print (df[\"streamflow_rate\"][2])\n",
    "        print (df[\"streamflow_rate\"][i_df])\n",
    "#         print (df_row)\n",
    "        if df[\"streamflow_rate\"][i_df] < avg_strmflw:\n",
    "            count = count + 1\n",
    "            \n",
    "            print (\"True\")\n",
    "        elif df[\"streamflow_rate\"][i_df] > avg_strmflw:\n",
    "            print (\"False\")\n",
    "        print(\"********************************************************************\")\n",
    "        print(\"********************************************************************\")\n",
    "\n",
    "    pct_blw_avg = (count / len(df) * 100)\n",
    "    print (pct_blw_avg)\n",
    "# Add a value into an empty dictionay element\n",
    "#     - https://www.pluralsight.com/guides/manipulating-lists-dictionaries-python\n",
    "    statn_data_lst_of_dicts[i].update({\"Data\": df})\n",
    "    statn_data_lst_of_dicts[i].update({\"Avg_Streamflow\": avg_strmflw})\n",
    "    statn_data_lst_of_dicts[i].update({\"Prcnt_Below_Avg\": pct_blw_avg})\n",
    "#     input_fle_dict[\"Data\"].append(df)\n",
    "\n",
    "    # df_nm = df_nm.drop(index = [0, 1])\n",
    "    # \"_\" + statn_nm + \"_df\" = df\n",
    "    # df_nm\n",
    "    # df\n",
    "    # df.drop(index = [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Next Step, take the total number of streams that are below its 38 year average and create a\n",
    "precent of that per day (calculate a percent by county then a total precent for the state (Use \n",
    "weighted averaging for the county and for the state, so that bigger streams have more weight in the\n",
    "precent)). This will tell us how many streams are below average per day and we can relate that to \n",
    "how many fires were reported that day and how many lightning strikes occured that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statn_data_lst_of_dicts\n",
    "# statn_data_lst_of_dicts[0]\n",
    "# statn_data_lst_of_dicts[0][\"Data\"]\n",
    "\n",
    "# data_df = statn_data_lst_of_dicts[0][\"Data\"]\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Refernces:\n",
    "#     - Get a list of dates between two dates\n",
    "#         - https://www.w3resource.com/python-exercises/date-time-exercise/python-date-time-exercise-50.php\n",
    "\n",
    "\n",
    "bgn_date_datetime = datetime.datetime.strptime(bgn_date, '%Y-%m-%d')\n",
    "end_date_datetime = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + timedelta(n)\n",
    "\n",
    "start_dt = date(2015, 12, 20)\n",
    "end_dt = date(2016, 1, 11)\n",
    "# print(type(end_dt_datetime ))\n",
    "\n",
    "for dt in daterange(bgn_date_datetime, end_date_datetime):\n",
    "    for \n",
    "    print(dt.strftime(\"%Y-%m-%d\")) # This is a String\n",
    "#     print(type(dt.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "        # Find the Index for the Station  \n",
    "    test_indx_lbl = data_df[data_df[\"date\"] == dt].index.tolist()\n",
    "\n",
    "    print(f'Index No: {test_indx_lbl[0]}')\n",
    "    print(f'Streamflow Rate: {data_df[\"streamflow_rate\"][test_indx_lbl[0]]}')\n",
    "    print(\"********************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Index for the Station  \n",
    "test_indx_lbl = data_df[data_df[\"date\"] == \"1990-01-05\"].index.tolist()\n",
    "\n",
    "print(f'Index No: {test_indx_lbl[0]}')\n",
    "print(f'Streamflow Rate: {data_df[\"streamflow_rate\"][test_indx_lbl[0]]}')\n",
    "\n",
    "\n",
    "\n",
    "# To Do:\n",
    "#     - Create a dataframe with the date, county name, total streamflow, average streamflow, weighted \n",
    "#         percent\n",
    "#     - Count how many Stations have a recorded streamflow for that day\n",
    "#         - Make and if statement: If the data exist then count_total else skip that station and go \n",
    "#             to the next station\n",
    "#         - If the data exist then sum the amount of water flowing in the streams for each county\n",
    "#             - Weighted precent for each stream per day\n",
    "#             - Use the count_county and weighted_average_percent to create a streamflow weighted \n",
    "#                 average per county (weighted average is how many county streams are below it's\n",
    "#                 extended average)\n",
    "#             - Add the streamflow rate to the state streamflow\n",
    "#             - Create streamflow weighted average for the state of Idaho using the county streamflow\n",
    "#                 (weighted average is how many county streams are below it's extended average)\n",
    "#             - Append the streamflow weighted average to the new dataframe\n",
    "#                  - The dataframe would include the date, streamflow weighted average, lightning\n",
    "#                      strikes, average prcp, average temp., average humdity, dew point, number of \n",
    "#                      camping permits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"_\" + input_fle_nm + \"_df\")\n",
    "# test = \"_\" + input_fle_nm\n",
    "# test[:-4]\n",
    "# input_fle_dict[\"df_Name\"]\n",
    "# _13073000_df.head()\n",
    "# _13206000_df.drop(index = [0, 1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_fle_dict[\"Data\"][0].append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_dict[\"Data\"][0].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_dict['Station_Nmbr'][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_dict[\"Data\"][index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# @@@@@@@@@@@@@@@@@@@@@@@"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import dropwhile\n",
    "\n",
    "input_fle_path = \"Data/Idaho_Streamflow_Data/\" + input_fle_nm\n",
    "\n",
    "count = 0 \n",
    "\n",
    "with open(input_fle_path,'r') as fh:\n",
    "    for curline in dropwhile(is_comment, fh):\n",
    "        print(f\"Index Number: {count} {curline}\")\n",
    "        count = count + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe for the Data\n",
    "\n",
    "clmn_nms = [\"agency\", \"site_nmbr\", \"date\", \"streamflow_rate\", \"approved/pending\"]\n",
    "\n",
    "_13206000_df = pd.DataFrame(columns = clmn_nms)\n",
    "\n",
    "_13206000_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a String\n",
    "#     - https://www.geeksforgeeks.org/python-string-split/\n",
    "# Pandas Series\n",
    "#     - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html\n",
    "\n",
    "print(type(curline))\n",
    "print(curline)\n",
    "print((curline.split(\"\\t\")))\n",
    "print(type(curline.split(\"\\t\")))\n",
    "\n",
    "to_append = curline[:-1].split(\"\\t\")\n",
    "a_series = pd.Series(to_append, index = clmn_nms)\n",
    "_13206000_df = _13206000_df.append(a_series, ignore_index=True)\n",
    "_13206000_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Get the Lighting Data from the National Centers for Enviromental Information (NCEI) National Oceanic and Atmospheric Administration (NOAA) Severe Weather Data Inventory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a webpage and create a BeautifulSoup object from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Create the Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import Select\n",
    "\n",
    "url = \"https://www.ncdc.noaa.gov/severe-weather/severe-weather-data-inventory\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/12323403/how-do-i-find-an-element-that-contains-specific-text-in-selenium-webdriver-pyth\n",
    "# https://selenium-python.readthedocs.io/locating-elements.html\n",
    "\n",
    "driver.find_element_by_xpath(\"//*[contains(text(), 'Map Search')]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "year = \"2001\"\n",
    "\n",
    "\n",
    "WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//input[@class='esri-input esri-search__input'])[1]\"))).send_keys(\"Idaho, USA\")\n",
    "# time.sleep(5)\n",
    "WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@class='esri-search__submit-button esri-widget--button'])[1]\"))).click()\n",
    "# time.sleep(10)\n",
    "# WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@id='yearSelect']/option[text()=\" + year + \"])\"))).click()\n",
    "# WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@class='custom-select swdi-select']/option[text()=\" + dataset + \"])\"))).click()\n",
    "\n",
    "# # https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "# driver.find_element_by_xpath(\"//select[@id='yearSelect']/option[text()=\" + year + \"]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@id='yearSelect']/option[text()=\" + year + \"])\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "#     - Get All the Options in the Dropdown List:\n",
    "#         - https://www.edureka.co/community/53559/how-get-all-options-dropdown-using-python-selenium-webdriver\n",
    "#     - Remove a List of Unwanted Characters from a String:\n",
    "#         - https://www.geeksforgeeks.org/python-removing-unwanted-characters-from-string/\n",
    "\n",
    "\n",
    "yrs_lghtnng_strks = [\"1992\", \"1993\", \"1994\", \"1995\", \"1996\", \"1997\", \"1998\", \"1999\", \"2000\", \"2001\", \"2002\", \"2003\", \"2004\", \"2005\", \"2006\", \"2007\", \"2008\", \"2009\", \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"]\n",
    "dataset = \"Lightning Strikes\"\n",
    "\n",
    "lghtnng_strks_df = pd.DataFrame()\n",
    "bad_chars = ['(', 'events)']\n",
    "\n",
    "for yr in yrs_lghtnng_strks:\n",
    "    WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@id='yearSelect']/option[text()=\" + yr + \"])\"))).click()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    select = Select(driver.find_element_by_id(\"datasetSelect\"))\n",
    "    select.select_by_visible_text(dataset)    \n",
    "    \n",
    "    time.sleep(15)\n",
    "    \n",
    "    lghtnng_strks = driver.find_element_by_id(\"dateSelect\")\n",
    "    options = [x for x in lghtnng_strks.find_elements_by_tag_name(\"option\")]\n",
    "    \n",
    "    for element in options:\n",
    "#     print (element.get_attribute(\"text\"))\n",
    "        text = element.get_attribute(\"text\")\n",
    "\n",
    "        count = 1\n",
    "    \n",
    "        for i in bad_chars:\n",
    "            text = text.replace(i, \"\")\n",
    "            \n",
    "            if count == 2:\n",
    "                text = text.replace(i, \"\")\n",
    "#                 print (text.split())\n",
    "#                 print (text.split()[0])\n",
    "#                 print (text.split()[1])\n",
    "#                 print (\"**************************\")\n",
    "                count = 0\n",
    "\n",
    "                lghtnng_strks_df = lghtnng_strks_df.append({\"date\": text.split()[0], \n",
    "                                                            \"number_of_strikes\": text.split()[1]}, ignore_index = True)\n",
    "\n",
    "            count = 1 + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = lghtnng_strks_df\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many seconds of phone calls are recorded in total?\n",
    "# print(test_df['number_of_strikes'].sum())\n",
    "# test_df\n",
    "\n",
    "# test_df.groupby(['month']).groups.keys()\n",
    "\n",
    "# test_df.groupby([test_df[\"date\"].dt.month]).sum().reset_index()\n",
    "\n",
    "\n",
    "# Split the String into Just the Year-Month:\n",
    "#     - https://stackoverflow.com/questions/26646191/pandas-groupby-month-and-year\n",
    "\n",
    "def getYearMonth(s):\n",
    "  return s.split(\"-\")[0]+\"-\"+s.split(\"-\")[1]\n",
    "\n",
    "test_df['YearMonth']= test_df['date'].apply(lambda x: getYearMonth(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.dtypes)\n",
    "\n",
    "# Change the Date Column to a datetime Data Type\n",
    "# test_df['date']= pd.to_datetime(test_df['date'])\n",
    "# or\n",
    "# test_df.astype({'date': 'datetime64'})\n",
    "\n",
    "# Change the \"number_of_strikes\" Column to an Integer (\"int32\") Data Type\n",
    "test_df = test_df.astype({'number_of_strikes': 'int32'})\n",
    "print(\"*******************************************\")\n",
    "print(test_df.dtypes)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_YearMonth_df = test_df.groupby(\"YearMonth\")[\"number_of_strikes\"].sum()\n",
    "test_YearMonth_df = pd.DataFrame(test_YearMonth_df)\n",
    "test_YearMonth_df = test_YearMonth_df.reset_index()\n",
    "test_YearMonth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@id='datasetSelect'])\"))).click()\n",
    "# WebDriverWait(driver,30).until(EC.visibility_of_element_located((By.XPATH, \"(//*[@id='datasetSelect']/option[text()=\" + dataset + \"])\"))).click()\n",
    "# driver.find_element_by_xpath(\"//select[@id='datasetSelect']/option[text()=\" + dataset + \"]\").click()\n",
    "# driver.find_element_by_xpath(\"//*[@id='datasetSelect']/option[text()=\" + dataset + \"]\").click()\n",
    "# driver.find_element_by_xpath(\"//*[@id='datasetSelect']\").click()\n",
    "\n",
    "# https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "select = Select(driver.find_element_by_id(\"datasetSelect\"))\n",
    "\n",
    "select.select_by_visible_text(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_YearMonth_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "#     Shading an area between two points in a matplotlib plot:\n",
    "#         - https://stackoverflow.com/questions/3681872/shading-an-area-between-two-points-in-a-matplotlib-plot\n",
    "\n",
    "\n",
    "x_axis = np.arange(len(test_YearMonth_df))\n",
    "\n",
    "plt.figure(figsize = (25,20))\n",
    "plt.bar(x_axis, test_YearMonth_df[\"number_of_strikes\"])\n",
    "plt.xticks(x_axis, test_YearMonth_df[\"YearMonth\"], rotation = \"vertical\")\n",
    "plt.hlines(10,0,92, alpha = 1, color = \"red\")\n",
    "plt.axvspan(0, 3, color='y', alpha=0.4, lw=0) # Highlighting the 1992 Lightning Strikes\n",
    "plt.axvspan(4, 8, color='g', alpha=0.4, lw=0) # Highlighting the 1993 Lightning Strikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.edureka.co/community/53559/how-get-all-options-dropdown-using-python-selenium-webdriver\n",
    "\n",
    "# lghtnng_strks = driver.find_element_by_id(\"dateSelect\")\n",
    "\n",
    "# options = [x for x in lghtnng_strks.find_elements_by_tag_name(\"option\")]\n",
    "\n",
    "# print(options)\n",
    "\n",
    "# for element in options:\n",
    "#     print (element.get_attribute(\"text\").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "#     - Get All the Options in the Dropdown List:\n",
    "#         - https://www.edureka.co/community/53559/how-get-all-options-dropdown-using-python-selenium-webdriver\n",
    "#     - Remove a List of Unwanted Characters from a String:\n",
    "#         - https://www.geeksforgeeks.org/python-removing-unwanted-characters-from-string/\n",
    "\n",
    "lghtnng_strks_df = pd.DataFrame()\n",
    "\n",
    "lghtnng_strks = driver.find_element_by_id(\"dateSelect\")\n",
    "\n",
    "options = [x for x in lghtnng_strks.find_elements_by_tag_name(\"option\")]\n",
    "\n",
    "bad_chars = ['(', 'events)']\n",
    "\n",
    "for element in options:\n",
    "#     print (element.get_attribute(\"text\"))\n",
    "    text = element.get_attribute(\"text\")\n",
    "    \n",
    "    count = 1\n",
    "    \n",
    "    for i in bad_chars:\n",
    "        text = text.replace(i, \"\")\n",
    "        \n",
    "        if count == 2:\n",
    "            text = text.replace(i, \"\")\n",
    "            print (text.split())\n",
    "            print (text.split()[0])\n",
    "            print (text.split()[1])\n",
    "            print (\"**************************\")\n",
    "            count = 0\n",
    "            \n",
    "            lghtnng_strks_df = lghtnng_strks_df.append({\"date\": text.split()[0], \n",
    "                                                        \"number_of_strikes\": text.split()[1]}, ignore_index = True)\n",
    "            \n",
    "        count = 1 + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lghtnng_strks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dropdown_value(year):\n",
    "    # https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "    driver.find_element_by_xpath(\"//select[@id='yearSelect']/option[text()='2001']\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://pythonspot.com/selenium-textbox/\n",
    "\n",
    "text_area = driver.find_element_by_class_name('esri-input esri-search__input')\n",
    "text_area.send_keys(\"This text is send using Python code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52873433/python-selenium-clicking-based-on-alt-attribute\n",
    "\n",
    "driver.find_element_by_css_selector('[alt=\"id\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press/Click a Button Without an ID\n",
    "#     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "\n",
    "lst_all_statns = '//input[@type=\"radio\" and @value=\"statelist\"]'\n",
    "\n",
    "button = driver.find_element_by_xpath(lst_all_statns)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "\n",
    "select = Select(driver.find_element_by_id('select_display'))\n",
    "\n",
    "# Select by visible text\n",
    "# select.select_by_visible_text('Daily Stage and Streamflow')\n",
    "\n",
    "# Select by value text\n",
    "select.select_by_value('dailystagedischarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Get the Mean Streamflow Rate for Each Station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a webpage and create a BeautifulSoup object from the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 USGS' Science for a Changing World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_fle_dict = {\"Station_Nmbr\":[], \"File_Name\": [], \"df_Name\": [], \"Data\": [], \"Avg_Streamflow\": []}\n",
    "\n",
    "input_fle_dict[\"Data\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_fle_dict[\"Data\"][0][\"streamflow_rate\"].mean(axis = 0)\n",
    "statn_data_lst_of_dicts[0][\"Data\"][\"streamflow_rate\"].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_dict[\"Station_Nmbr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of a dictionary within a list (I modified this codes since my dictionary isn't in a list)\n",
    "#     - https://stackoverflow.com/questions/4391697/find-the-index-of-a-dict-within-a-list-by-matching-the-dicts-value\n",
    "\n",
    "def find_avg (lst, key, value):\n",
    "    for i, a_dict_2 in enumerate(lst):\n",
    "        print(a_dict_2[key])\n",
    "    #         print(input_fle_dict[\"Station_Nmbr\"])\n",
    "    #         print(\"********************************************************************\")\n",
    "#         if a_dict_2[key] == value:    \n",
    "#         if station == value:\n",
    "#             print(i)\n",
    "    \n",
    "                \n",
    "    \n",
    "#             avg_strmflw_rte = dict_nm[\"Data\"][i][\"streamflow_rate\"].mean(axis = 0)\n",
    "#             dict_nm[\"Avg_Streamflow\"][i].append(avg_strmflw_rte )\n",
    "\n",
    "#             avg_strmflw = lst[i][\"Data\"][\"streamflow_rate\"].mean(axis = 0)\n",
    "#             lst[i].update({\"Avg_Streamflow\": avg_strmflw})\n",
    "        \n",
    "#             return i # avg_strmflw_rte\n",
    "    #             print(\"********************************************************************\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fle_dict[\"Station_Nmbr\"][0].append(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_avg(statn_data_lst_of_dicts, \"Station_Nmbr\", \"13206000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst = [{'id':'1234','name':'Jason'}, {'id':'2345','name':'Tom'}, {'id':'3456','name':'Art'}]\n",
    "\n",
    "tom_index = next((index for (index, d) in enumerate(lst) if d[\"name\"] == \"Tom\"), None)\n",
    "tom_index\n",
    "\n",
    "# tom_index = next((index for (index, d) in enumerate(input_fle_dict) if d[\"Station_Nmbr\"] == 13075910), None)\n",
    "# print(tom_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types1 = [type(k) for k in input_fle_dict[\"Station_Nmbr\"]]\n",
    "types1\n",
    "# type(13075000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = [{'id':'1234','name':'Jason'},\n",
    "         {'id':'2345','name':'Tom'},\n",
    "         {'id':'3456','name':'Art'}]\n",
    "\n",
    "def find_index(dicts, key, value):\n",
    "    class Null: pass\n",
    "    for i, d in enumerate(dicts):\n",
    "        if d.get(key, Null) == value:\n",
    "            return d\n",
    "    else:\n",
    "        raise ValueError('no dict with the key and value combination found')\n",
    "\n",
    "print (find_index(dicts, 'name', 'Tom'))\n",
    "# 1\n",
    "# find_index(dicts, 'name', 'Ensnare')\n",
    "# ValueError: no dict with the key and value combination found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(lst, key, value):\n",
    "#     i = 0\n",
    "    for i, dic in enumerate(lst):\n",
    "        print(lst)\n",
    "        print(dic)\n",
    "        print(\"********************************************************************\")\n",
    "        if dic[key] == value:\n",
    "            return i\n",
    "            print(\"********************************************************************\")\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find(lst, \"name\", \"Tom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using list comprehension + enumerate() \n",
    "# Key index in Dictionary \n",
    "search_key = \"13075000\"\n",
    "\n",
    "temp = list(input_fle_dict.items())  Station_Nmbr\n",
    "res = list(input_fle_dict.keys()).index(search_key) \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_fle_nm in input_fle_dict[\"File_Name\"]:\n",
    "    \n",
    "# Using list comprehension + enumerate() \n",
    "# Key index in Dictionary \n",
    "    temp = list(test_dict.items())  \n",
    "    res = [idx for idx, key in enumerate(temp) if key[0] == search_key] \n",
    "    \n",
    "    \n",
    "    input_fle_dict[\"Data\"][0][\"streamflow_rate\"] = input_fle_dict[\"Data\"][0][\"streamflow_rate\"].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open(\"Data/Idaho_Streamflow_Data/13206000.txt\", \"r\")\n",
    "lines = file.readlines()[26:]\n",
    "\n",
    "print(type(lines))\n",
    "print(lines)\n",
    "\n",
    "pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a Row from the List\n",
    "#     - https://note.nkmk.me/en/python-list-clear-pop-remove-del/#:~:text=In%20Python%2C%20use%20list%20methods,with%20an%20index%20or%20slice.\n",
    "\n",
    "del lines[0:1]\n",
    "lines\n",
    "# print((lines[1].split(\"\\t\")))\n",
    "test = lines[1][:-1].split(\"\\t\")\n",
    "test\n",
    "\n",
    "a_series = pd.Series(test, index = clmn_nms)\n",
    "_13206000_df = _13206000_df.append(a_series, ignore_index=True)\n",
    "_13206000_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the Text File and Convert to a Dataframe\n",
    "data = pd.read_csv('Data/Idaho_Streamflow_Data/13206000.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Empty Rows\n",
    "import numpy as np\n",
    "np.where(pd.isnull(statn_table_df_splt_StatnNmbr_nmbrs_clmn))\n",
    "# statn_table_df_splt_StatnNmbr_nmbrs_clmn.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df[\"StationNumber\"] = statn_table_df[\"StationNumber\"].astype(str)\n",
    "print(statn_table.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if \"County\" in statn_table[\"StationNumber\"][0]:\n",
    "        print(\"true\")\n",
    "        statn_table_df_drp_cnty = statn_table_df.drop([0, 4], axis = 0)\n",
    "statn_table_df_drp_cnty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the County Names from the Table\n",
    "# test = statn_table.drop([0, 4], axis = 0)\n",
    "# test.head()\n",
    "\n",
    "# Count the number of Rows in the Dataframe\n",
    "count = 0\n",
    "for statn_table_df_row in statn_table_df.index:\n",
    "\n",
    "    if \"County\" in statn_table_df[\"StationNumber\"][statn_table_df_row]:\n",
    "        statn_table_df_drp_cnty = statn_table_df.drop([statn_table_df_row], axis = 0)\n",
    "        count = count + 1\n",
    "\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df_drp_cnty.head(15)\n",
    "# statn_table_df_drp_cnty.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statn_table_df_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statn_table_df[\"StationNumber\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab All Page Source on the Page\n",
    "soup_lxml = BS(driver.page_source, \"lxml\")\n",
    "\n",
    "# Find All the Tables on the Page\n",
    "tables = soup_lxml.find_all(\"table\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Tables with Pandas\n",
    "dfs = pd.read_html(str(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the Table\n",
    "print(f\"Number of Tables on the page: {len(dfs)}\")\n",
    "print(\"*********************************************************************************************************\")\n",
    "print(f\"Data Types for the Table: {dfs[11].dtypes}\")\n",
    "print(\"*********************************************************************************************************\")\n",
    "print(f\"Number of Rows in the dataframe: {len(dfs[11])}\")\n",
    "dfs[11].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[11][\"USGSstationnumber\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press/Click a Button Without an ID\n",
    "#     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "\n",
    "# Select(driver.find_element_by_id('rdb'))\n",
    "\n",
    "tab_sprtd_rado = '//input[@type=\"radio\" and @value=\"rdb\"]'\n",
    "\n",
    "button = driver.find_element_by_xpath(tab_sprtd_rado)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify location of chromedriver and store it as a variable\n",
    "chromedriver = !which chromedriver\n",
    "print(type(chromedriver))\n",
    "chromedriver[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Retrieve the data/information on USGS' WaterWatch website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "executable_path = {\"executable_path\": \"chromedriver\"}\n",
    "# OR\n",
    "# executable_path = {\"executable_path\": chromedriver[0]}\n",
    "# I am not sure why the above works and the below statement will not. I think it's b/c chromebriver is a class 'IPython.utils.text.SList'?\n",
    "# executable_path = {\"executable_path\": chromedriver}\n",
    "\n",
    "browser = Browser('chrome', **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = \"https://waterwatch.usgs.gov/index.php?id=wwdrought\"\n",
    "# url = \"https://www.usgs.gov/\"\n",
    "browser.visit(url)\n",
    "window = browser.windows.current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BS(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the html code of the NASA's Mars website\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19392466/python-beautifulsoup-get-select-value-not-text\n",
    "\n",
    "for option in soup.find_all('option'):\n",
    "    print(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/7867537/how-to-select-a-drop-down-menu-value-with-selenium-using-python\n",
    "\n",
    "select = Select(driver.find_element_by_id('st'))\n",
    "\n",
    "# Select by visible text\n",
    "select.select_by_visible_text('Idaho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in Input Fills\n",
    "#     - https://stackoverflow.com/questions/25537567/how-to-open-website-and-fill-in-input-using-selenium-webdriver\n",
    "# Clear the Input Field\n",
    "#     - http://10minbasics.com/clear-fill-input-field-with-selenium/\n",
    "\n",
    "element = driver.find_element_by_name(\"bdt\")\n",
    "element.clear()\n",
    "element.send_keys(\"1990-01-01\")\n",
    "\n",
    "element = driver.find_element_by_name(\"edt\")\n",
    "element.clear()\n",
    "element.send_keys(\"1990-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press/Click a Button Without an ID\n",
    "#     - https://stackoverflow.com/questions/8871654/how-to-press-click-the-button-using-selenium-if-the-button-does-not-have-the-id\n",
    "\n",
    "NEXT_BUTTON_XPATH = '//input[@type=\"submit\" and @value=\"GO\"]'\n",
    "\n",
    "button = driver.find_element_by_xpath(NEXT_BUTTON_XPATH)\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5041008/how-to-find-elements-by-class\n",
    "\n",
    "# test = soup.find_all(\"div\", class_= \"ztable\")\n",
    "# test\n",
    "\n",
    "# https://stackoverflow.com/questions/20522820/how-to-get-tbody-from-table-from-python-beautiful-soup\n",
    "soup.findAll('table')[0].findAll('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab All Page Source on the Page\n",
    "soup_lxml = BS(driver.page_source, \"lxml\")\n",
    "\n",
    "# Find All the Tables on the Page\n",
    "tables = soup_lxml.find_all(\"table\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Tables with Pandas\n",
    "dfs = pd.read_html(str(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the Table\n",
    "print(f\"Number of Tables on the page: {len(dfs)}\")\n",
    "print(\"*********************************************************************************************************\")\n",
    "print(f\"Data Types for the Table: {dfs[11].dtypes}\")\n",
    "print(\"*********************************************************************************************************\")\n",
    "print(f\"Number of Rows in the dataframe: {len(dfs[11])}\")\n",
    "dfs[11].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs[11][\"USGSstationnumber\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "text = requests.get(\"https://waterwatch.usgs.gov/index.php?id=wwdrought\").text\n",
    "data = json.loads(text)\n",
    "print(data['Scty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in soup.find_all('tr')[2:]:\n",
    "    tds = tr.find_all('td')\n",
    "    print (tds)#\"Nome: %s, Cognome: %s, Email: %s\" % \\\n",
    "#           (tds[0].text, tds[1].text, tds[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.1 Collect the latest News Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
